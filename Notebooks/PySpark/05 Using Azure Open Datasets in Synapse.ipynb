{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using Azure Open Datasets in Synapse - Enrich NYC Green Taxi Data with Holiday and Weather\n",
        "\n",
        "Synapse has [Azure Open Datasets](https://azure.microsoft.com/en-us/services/open-datasets/) package pre-installed. This notebook provides examples of how to enrich NYC Green Taxi Data with Holiday and Weather with focusing on :\n",
        "- read Azure Open Dataset\n",
        "- manipulate the data to prepare for further analysis, including column projection, filtering, grouping and joins etc. \n",
        "- create a Spark table to be used in other notebooks for modeling training"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading \n",
        "Let's first load the [NYC green taxi trip records](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/). The Open Datasets package contains a class representing each data source (NycTlcGreen for example) to easily filter date parameters before downloading."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import NycTlcGreen\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "end_date = parser.parse('2018-06-06')\n",
        "start_date = parser.parse('2018-05-01')\n",
        "\n",
        "nyc_tlc = NycTlcGreen(start_date=start_date, end_date=end_date)\n",
        "nyc_tlc_df = nyc_tlc.to_spark_dataframe()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------+-------------------+-------------------+--------------+------------+------------+------------+---------------+--------------+----------------+---------------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+--------+-----------+--------+------+-------+\n|vendorID|lpepPickupDatetime |lpepDropoffDatetime|passengerCount|tripDistance|puLocationId|doLocationId|pickupLongitude|pickupLatitude|dropoffLongitude|dropoffLatitude|rateCodeID|storeAndFwdFlag|paymentType|fareAmount|extra|mtaTax|improvementSurcharge|tipAmount|tollsAmount|ehailFee|totalAmount|tripType|puYear|puMonth|\n+--------+-------------------+-------------------+--------------+------------+------------+------------+---------------+--------------+----------------+---------------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+--------+-----------+--------+------+-------+\n|2       |2018-06-02 14:10:02|2018-06-02 14:30:23|1             |2.5         |41          |247         |null           |null          |null            |null           |1         |N              |1          |14.5      |0.0  |0.5   |0.3                 |3.06     |0.0        |null    |18.36      |1       |2018  |6      |\n|2       |2018-06-02 14:36:36|2018-06-02 14:41:11|1             |0.45        |42          |42          |null           |null          |null            |null           |1         |N              |2          |5.0       |0.0  |0.5   |0.3                 |0.0      |0.0        |null    |5.8        |1       |2018  |6      |\n|2       |2018-06-04 11:18:01|2018-06-04 11:20:58|1             |0.8         |74          |74          |null           |null          |null            |null           |1         |N              |2          |4.5       |0.0  |0.5   |0.3                 |0.0      |0.0        |null    |5.3        |1       |2018  |6      |\n|2       |2018-06-02 17:47:28|2018-06-02 18:01:06|1             |1.68        |74          |42          |null           |null          |null            |null           |1         |N              |2          |10.5      |0.0  |0.5   |0.3                 |0.0      |0.0        |null    |11.3       |1       |2018  |6      |\n|1       |2018-06-02 17:24:06|2018-06-02 17:44:21|3             |6.9         |93          |131         |null           |null          |null            |null           |1         |N              |1          |22.0      |0.0  |0.5   |0.3                 |3.42     |0.0        |null    |26.22      |1       |2018  |6      |\n+--------+-------------------+-------------------+--------------+------------+------------+------------+---------------+--------------+----------------+---------------+----------+---------------+-----------+----------+-----+------+--------------------+---------+-----------+--------+-----------+--------+------+-------+\nonly showing top 5 rows"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "\n",
        "nyc_tlc_df.show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the initial data is loaded. Let's do some projection on the data to \n",
        "- create new columns for the month number, day of month, day of week, and hour of day. These info is going to be used in the training model to factor in time-based seasonality.\n",
        "- add a static feature for the country code to join holiday data. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Extract month, day of month, and day of week from pickup datetime and add a static column for the country code to join holiday data. \n",
        "\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "nyc_tlc_df_expand = nyc_tlc_df.withColumn('datetime',f.to_date('lpepPickupDatetime'))\\\n",
        "                .withColumn('month_num',f.month(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('day_of_month',f.dayofmonth(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('day_of_week',f.dayofweek(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('hour_of_day',f.hour(nyc_tlc_df.lpepPickupDatetime))\\\n",
        "                .withColumn('country_code',f.lit('US'))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remove some of the columns that won't need for modeling or additional feature building.\n",
        "\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Remove unused columns from nyc green taxi data\n",
        "\n",
        "columns_to_remove = [\"lpepDropoffDatetime\", \"puLocationId\", \"doLocationId\", \"pickupLongitude\", \n",
        "                     \"pickupLatitude\", \"dropoffLongitude\",\"dropoffLatitude\" ,\"rateCodeID\", \n",
        "                     \"storeAndFwdFlag\",\"paymentType\", \"fareAmount\", \"extra\", \"mtaTax\",\n",
        "                     \"improvementSurcharge\", \"tollsAmount\", \"ehailFee\", \"tripType \"  \n",
        "                    ]\n",
        "\n",
        "nyc_tlc_df_clean = nyc_tlc_df_expand.select([column for column in nyc_tlc_df_expand.columns if column not in columns_to_remove])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+----------+---------+------------+-----------+-----------+------------+\n|vendorID| lpepPickupDatetime|passengerCount|tripDistance|tipAmount|totalAmount|tripType|puYear|puMonth|  datetime|month_num|day_of_month|day_of_week|hour_of_day|country_code|\n+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+----------+---------+------------+-----------+-----------+------------+\n|       2|2018-06-02 14:10:02|             1|         2.5|     3.06|      18.36|       1|  2018|      6|2018-06-02|        6|           2|          7|         14|          US|\n|       2|2018-06-02 14:36:36|             1|        0.45|      0.0|        5.8|       1|  2018|      6|2018-06-02|        6|           2|          7|         14|          US|\n|       2|2018-06-04 11:18:01|             1|         0.8|      0.0|        5.3|       1|  2018|      6|2018-06-04|        6|           4|          2|         11|          US|\n|       2|2018-06-02 17:47:28|             1|        1.68|      0.0|       11.3|       1|  2018|      6|2018-06-02|        6|           2|          7|         17|          US|\n|       1|2018-06-02 17:24:06|             3|         6.9|     3.42|      26.22|       1|  2018|      6|2018-06-02|        6|           2|          7|         17|          US|\n+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+----------+---------+------------+-----------+-----------+------------+\nonly showing top 5 rows"
          },
          "execution_count": 7,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "nyc_tlc_df_clean.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enrich with holiday data\n",
        "Now that we have taxi data downloaded and roughly prepared, add in holiday data as additional features. Holiday-specific features will assist model accuracy, as major holidays are times where taxi demand increases dramatically and supply becomes limited. \n",
        "\n",
        "Let's load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\n|countryOrRegion|holidayName                 |normalizeHolidayName        |isPaidTimeOff|countryRegionCode|date               |\n+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\n|Argentina      |Día del Trabajo [Labour Day]|Día del Trabajo [Labour Day]|null         |AR               |2018-05-01 00:00:00|\n|Austria        |Staatsfeiertag              |Staatsfeiertag              |null         |AT               |2018-05-01 00:00:00|\n|Belarus        |Праздник труда              |Праздник труда              |null         |BY               |2018-05-01 00:00:00|\n|Belgium        |Dag van de Arbeid           |Dag van de Arbeid           |null         |BE               |2018-05-01 00:00:00|\n|Brazil         |Dia Mundial do Trabalho     |Dia Mundial do Trabalho     |null         |BR               |2018-05-01 00:00:00|\n+---------------+----------------------------+----------------------------+-------------+-----------------+-------------------+\nonly showing top 5 rows"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()\n",
        "\n",
        "# Display data\n",
        "hol_df.show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rename the countryRegionCode and date columns to match the respective field names from the taxi data, and also normalize the time so it can be used as a key. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+---------------+--------------------+--------------------+-------------+------------+-------------------+----------+\n|countryOrRegion|         holidayName|normalizeHolidayName|isPaidTimeOff|country_code|               date|  datetime|\n+---------------+--------------------+--------------------+-------------+------------+-------------------+----------+\n|      Argentina|Día del Trabajo [...|Día del Trabajo [...|         null|          AR|2018-05-01 00:00:00|2018-05-01|\n|        Austria|      Staatsfeiertag|      Staatsfeiertag|         null|          AT|2018-05-01 00:00:00|2018-05-01|\n|        Belarus|      Праздник труда|      Праздник труда|         null|          BY|2018-05-01 00:00:00|2018-05-01|\n|        Belgium|   Dag van de Arbeid|   Dag van de Arbeid|         null|          BE|2018-05-01 00:00:00|2018-05-01|\n|         Brazil|Dia Mundial do Tr...|Dia Mundial do Tr...|         null|          BR|2018-05-01 00:00:00|2018-05-01|\n+---------------+--------------------+--------------------+-------------+------------+-------------------+----------+\nonly showing top 5 rows"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "hol_df_clean = hol_df.withColumnRenamed('countryRegionCode','country_code')\\\n",
        "            .withColumn('datetime',f.to_date('date'))\n",
        "\n",
        "hol_df_clean.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, join the holiday data with the taxi data by performing a left-join. This will preserve all records from taxi data, but add in holiday data where it exists for the corresponding datetime and country_code, which in this case is always \"US\". Preview the data to verify that they were merged correctly."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+-----------+--------------------+-------------+----+\n|  datetime|country_code|vendorID| lpepPickupDatetime|passengerCount|tripDistance|tipAmount|totalAmount|tripType|puYear|puMonth|month_num|day_of_month|day_of_week|hour_of_day|countryOrRegion|holidayName|normalizeHolidayName|isPaidTimeOff|date|\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+-----------+--------------------+-------------+----+\n|2018-06-02|          US|       2|2018-06-02 14:10:02|             1|         2.5|     3.06|      18.36|       1|  2018|      6|        6|           2|          7|         14|           null|       null|                null|         null|null|\n|2018-06-02|          US|       2|2018-06-02 14:36:36|             1|        0.45|      0.0|        5.8|       1|  2018|      6|        6|           2|          7|         14|           null|       null|                null|         null|null|\n|2018-06-04|          US|       2|2018-06-04 11:18:01|             1|         0.8|      0.0|        5.3|       1|  2018|      6|        6|           4|          2|         11|           null|       null|                null|         null|null|\n|2018-06-02|          US|       2|2018-06-02 17:47:28|             1|        1.68|      0.0|       11.3|       1|  2018|      6|        6|           2|          7|         17|           null|       null|                null|         null|null|\n|2018-06-02|          US|       1|2018-06-02 17:24:06|             3|         6.9|     3.42|      26.22|       1|  2018|      6|        6|           2|          7|         17|           null|       null|                null|         null|null|\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+-----------+--------------------+-------------+----+\nonly showing top 5 rows"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# enrich taxi data with holiday data\n",
        "nyc_taxi_holiday_df = nyc_tlc_df_clean.join(hol_df_clean, on = ['datetime', 'country_code'] , how = 'left')\n",
        "\n",
        "nyc_taxi_holiday_df.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+\n|datetime  |country_code|vendorID|lpepPickupDatetime |passengerCount|tripDistance|tipAmount|totalAmount|tripType|puYear|puMonth|month_num|day_of_month|day_of_week|hour_of_day|countryOrRegion|holidayName |normalizeHolidayName|isPaidTimeOff|date               |\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+\n|2018-05-28|US          |2       |2018-05-28 10:28:09|1             |2.01        |2.26     |13.56      |1       |2018  |5      |5        |28          |2          |10         |United States  |Memorial Day|Memorial Day        |true         |2018-05-28 00:00:00|\n|2018-05-28|US          |2       |2018-05-28 11:18:08|1             |1.23        |0.0      |7.3        |1       |2018  |5      |5        |28          |2          |11         |United States  |Memorial Day|Memorial Day        |true         |2018-05-28 00:00:00|\n|2018-05-28|US          |2       |2018-05-28 13:07:12|1             |71.23       |0.0      |181.8      |1       |2018  |5      |5        |28          |2          |13         |United States  |Memorial Day|Memorial Day        |true         |2018-05-28 00:00:00|\n|2018-05-28|US          |2       |2018-05-28 00:02:29|1             |0.87        |0.0      |6.8        |1       |2018  |5      |5        |28          |2          |0          |United States  |Memorial Day|Memorial Day        |true         |2018-05-28 00:00:00|\n|2018-05-28|US          |2       |2018-05-28 00:05:18|1             |6.54        |0.0      |24.3       |1       |2018  |5      |5        |28          |2          |0          |United States  |Memorial Day|Memorial Day        |true         |2018-05-28 00:00:00|\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+\nonly showing top 5 rows"
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Create a temp table and filter out non empty holiday rows\n",
        "\n",
        "nyc_taxi_holiday_df.createOrReplaceTempView(\"nyc_taxi_holiday_df\")\n",
        "spark.sql(\"SELECT * from nyc_taxi_holiday_df WHERE holidayName is NOT NULL \").show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enrich with weather data\n",
        "\n",
        "Now we append NOAA surface weather data to the taxi and holiday data. Use a similar approach to fetch the [NOAA weather history data](https://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/) from Azure Open Datasets. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import NoaaIsdWeather\n",
        "\n",
        "isd = NoaaIsdWeather(start_date, end_date)\n",
        "isd_df = isd.to_spark_dataframe()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+-------------+---------------+------------+----+---+-------+-----+\n|usaf  |wban |datetime           |latitude|longitude|elevation|windAngle|windSpeed|temperature|seaLvlPressure|cloudCoverage|presentWeatherIndicator|pastWeatherIndicator|precipTime|precipDepth|snowDepth|stationName  |countryOrRegion|p_k         |year|day|version|month|\n+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+-------------+---------------+------------+----+---+-------+-----+\n|999999|53182|2018-05-26 07:55:00|36.568  |-101.61  |1000.0   |null     |null     |20.4       |null          |null         |null                   |null                |null      |null       |null     |null         |null           |999999-53182|2018|26 |1.0    |5    |\n|999999|03048|2018-05-20 05:10:00|34.356  |-106.886 |1477.0   |null     |null     |21.2       |null          |null         |null                   |null                |null      |null       |null     |SOCORRO 20 N |US             |999999-03048|2018|20 |1.0    |5    |\n|999999|03048|2018-05-15 13:15:00|34.356  |-106.886 |1477.0   |null     |null     |13.1       |null          |null         |null                   |null                |null      |null       |null     |SOCORRO 20 N |US             |999999-03048|2018|15 |1.0    |5    |\n|999999|03048|2018-05-01 21:20:00|34.356  |-106.886 |1477.0   |null     |null     |24.4       |null          |null         |null                   |null                |null      |null       |null     |SOCORRO 20 N |US             |999999-03048|2018|1  |1.0    |5    |\n|999999|03072|2018-05-09 02:05:00|32.041  |-100.25  |609.0    |null     |null     |29.2       |null          |null         |null                   |null                |null      |null       |null     |BRONTE 11 NNE|US             |999999-03072|2018|9  |1.0    |5    |\n+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+-------------+---------------+------------+----+---+-------+-----+\nonly showing top 5 rows"
          },
          "execution_count": 13,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "isd_df.show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Filter out weather info for new york city, remove the recording with null temperature \n",
        "\n",
        "weather_df = isd_df.filter(isd_df.latitude >= '40.53')\\\n",
        "                        .filter(isd_df.latitude <= '40.88')\\\n",
        "                        .filter(isd_df.longitude >= '-74.09')\\\n",
        "                        .filter(isd_df.longitude <= '-73.72')\\\n",
        "                        .filter(isd_df.temperature.isNotNull())\\\n",
        "                        .withColumnRenamed('datetime','datetime_full')\n",
        "                         "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-------------------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+-----+----------+\n|datetime_full      |elevation|windAngle|windSpeed|temperature|seaLvlPressure|cloudCoverage|presentWeatherIndicator|pastWeatherIndicator|precipTime|precipDepth|snowDepth|stationName       |countryOrRegion|p_k         |year|day|version|month|datetime  |\n+-------------------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+-----+----------+\n|2018-05-30 06:51:00|9.0      |50       |3.1      |17.8       |1017.1        |FEW          |null                   |null                |1.0       |0.0        |null     |LA GUARDIA AIRPORT|US             |725030-14732|2018|30 |1.0    |5    |2018-05-30|\n|2018-05-15 03:51:00|9.0      |180      |3.1      |15.6       |1013.8        |null         |null                   |null                |1.0       |0.0        |null     |LA GUARDIA AIRPORT|US             |725030-14732|2018|15 |1.0    |5    |2018-05-15|\n|2018-05-17 21:51:00|9.0      |210      |2.6      |22.8       |1016.7        |null         |null                   |null                |1.0       |0.0        |null     |LA GUARDIA AIRPORT|US             |725030-14732|2018|17 |1.0    |5    |2018-05-17|\n|2018-05-26 07:51:00|9.0      |210      |3.6      |23.9       |1009.5        |CLR          |null                   |null                |1.0       |0.0        |null     |LA GUARDIA AIRPORT|US             |725030-14732|2018|26 |1.0    |5    |2018-05-26|\n|2018-05-12 15:51:00|9.0      |null     |2.6      |14.4       |1019.6        |null         |null                   |null                |1.0       |0.0        |null     |LA GUARDIA AIRPORT|US             |725030-14732|2018|12 |1.0    |5    |2018-05-12|\n+-------------------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+-----+----------+\nonly showing top 5 rows"
          },
          "execution_count": 15,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Remove unused columns\n",
        "\n",
        "columns_to_remove_weather = [\"usaf\", \"wban\", \"longitude\", \"latitude\"]\n",
        "weather_df_clean = weather_df.select([column for column in weather_df.columns if column not in columns_to_remove_weather])\\\n",
        "                        .withColumn('datetime',f.to_date('datetime_full'))\n",
        "\n",
        "weather_df_clean.show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next group the weather data so that you have daily aggregated weather values. \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Enrich weather data with aggregation statistics\n",
        "\n",
        "aggregations = {\"snowDepth\": \"mean\", \"precipTime\": \"max\", \"temperature\": \"mean\", \"precipDepth\": \"max\"}\n",
        "weather_df_grouped = weather_df_clean.groupby(\"datetime\").agg(aggregations)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+----------+--------------+------------------+---------------+----------------+\n|  datetime|avg(snowDepth)|  avg(temperature)|max(precipTime)|max(precipDepth)|\n+----------+--------------+------------------+---------------+----------------+\n|2018-05-28|          null| 15.33363636363636|           24.0|          2540.0|\n|2018-06-06|          null|              21.4|            6.0|             0.0|\n|2018-05-26|          null|26.072330097087377|           24.0|          2540.0|\n|2018-05-27|          null| 18.93136531365314|           24.0|          7648.0|\n|2018-06-03|          null|18.242803030303037|           24.0|          2540.0|\n+----------+--------------+------------------+---------------+----------------+\nonly showing top 5 rows"
          },
          "execution_count": 17,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "weather_df_grouped.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Rename columns\n",
        "\n",
        "weather_df_grouped = weather_df_grouped.withColumnRenamed('avg(snowDepth)','avg_snowDepth')\\\n",
        "                                       .withColumnRenamed('avg(temperature)','avg_temperature')\\\n",
        "                                       .withColumnRenamed('max(precipTime)','max_precipTime')\\\n",
        "                                       .withColumnRenamed('max(precipDepth)','max_precipDepth')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Merge the taxi and holiday data you prepared with the new weather data. This time you only need the datetime key, and again perform a left-join of the data. Run the describe() function on the new dataframe to see summary statistics for each field."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "DataFrame[datetime: date, country_code: string, vendorID: int, lpepPickupDatetime: timestamp, passengerCount: int, tripDistance: double, tipAmount: double, totalAmount: double, tripType: int, puYear: int, puMonth: int, month_num: int, day_of_month: int, day_of_week: int, hour_of_day: int, countryOrRegion: string, holidayName: string, normalizeHolidayName: string, isPaidTimeOff: boolean, date: timestamp, avg_snowDepth: double, avg_temperature: double, max_precipTime: double, max_precipDepth: double]"
          },
          "execution_count": 19,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# enrich taxi data with weather\n",
        "nyc_taxi_holiday_weather_df = nyc_taxi_holiday_df.join(weather_df_grouped, on = 'datetime' , how = 'left')\n",
        "nyc_taxi_holiday_weather_df.cache()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+-------------+-----------------+--------------+---------------+\n|  datetime|country_code|vendorID| lpepPickupDatetime|passengerCount|tripDistance|tipAmount|totalAmount|tripType|puYear|puMonth|month_num|day_of_month|day_of_week|hour_of_day|countryOrRegion| holidayName|normalizeHolidayName|isPaidTimeOff|               date|avg_snowDepth|  avg_temperature|max_precipTime|max_precipDepth|\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+-------------+-----------------+--------------+---------------+\n|2018-05-28|          US|       2|2018-05-28 10:28:09|             1|        2.01|     2.26|      13.56|       1|  2018|      5|        5|          28|          2|         10|  United States|Memorial Day|        Memorial Day|         true|2018-05-28 00:00:00|         null|15.33363636363636|          24.0|         2540.0|\n|2018-05-28|          US|       2|2018-05-28 11:18:08|             1|        1.23|      0.0|        7.3|       1|  2018|      5|        5|          28|          2|         11|  United States|Memorial Day|        Memorial Day|         true|2018-05-28 00:00:00|         null|15.33363636363636|          24.0|         2540.0|\n|2018-05-28|          US|       2|2018-05-28 13:07:12|             1|       71.23|      0.0|      181.8|       1|  2018|      5|        5|          28|          2|         13|  United States|Memorial Day|        Memorial Day|         true|2018-05-28 00:00:00|         null|15.33363636363636|          24.0|         2540.0|\n|2018-05-28|          US|       2|2018-05-28 00:02:29|             1|        0.87|      0.0|        6.8|       1|  2018|      5|        5|          28|          2|          0|  United States|Memorial Day|        Memorial Day|         true|2018-05-28 00:00:00|         null|15.33363636363636|          24.0|         2540.0|\n|2018-05-28|          US|       2|2018-05-28 00:05:18|             1|        6.54|      0.0|       24.3|       1|  2018|      5|        5|          28|          2|          0|  United States|Memorial Day|        Memorial Day|         true|2018-05-28 00:00:00|         null|15.33363636363636|          24.0|         2540.0|\n+----------+------------+--------+-------------------+--------------+------------+---------+-----------+--------+------+-------+---------+------------+-----------+-----------+---------------+------------+--------------------+-------------+-------------------+-------------+-----------------+--------------+---------------+\nonly showing top 5 rows"
          },
          "execution_count": 20,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "nyc_taxi_holiday_weather_df.show(5)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/json": {
              "rows": [
                [
                  "count",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "923257",
                  "17606",
                  "17606",
                  "17606",
                  "0",
                  "923257",
                  "923257",
                  "923257"
                ],
                [
                  "mean",
                  null,
                  "1.833860994284365",
                  "1.357852688904606",
                  "3.2325295231988496",
                  "1.0264538909534446",
                  "16.112515713388355",
                  "1.0154323227443713",
                  "2018.0",
                  "5.1364462982679795",
                  "5.1364462982679795",
                  "13.844344532454127",
                  "4.137266221647927",
                  "13.697660564718166",
                  null,
                  null,
                  null,
                  null,
                  "18.581906658840314",
                  "19.26542663635369",
                  "4213.8777393510145"
                ],
                [
                  "stddev",
                  null,
                  "0.37220557028061513",
                  "1.0397434350543047",
                  "3.7213549381277544",
                  "2.074783792187912",
                  "13.49278907932152",
                  "0.12326468519506638",
                  "2.1388604802046966E-13",
                  "0.3432620479749415",
                  "0.3432620479749415",
                  "9.330046641186325",
                  "1.9820965262931112",
                  "5.987036976846432",
                  null,
                  null,
                  null,
                  null,
                  "3.6046653323635853",
                  "8.120658513433673",
                  "4389.942208506072"
                ],
                [
                  "min",
                  "US",
                  "1",
                  "0",
                  "0.0",
                  "-2.0",
                  "-235.0",
                  "1",
                  "2018",
                  "5",
                  "5",
                  "1",
                  "1",
                  "0",
                  "United States",
                  "Memorial Day",
                  "Memorial Day",
                  null,
                  "11.900000000000006",
                  "1.0",
                  "0.0"
                ],
                [
                  "max",
                  "US",
                  "2",
                  "9",
                  "621.1",
                  "450.0",
                  "2704.8",
                  "2",
                  "2018",
                  "6",
                  "6",
                  "31",
                  "7",
                  "23",
                  "United States",
                  "Memorial Day",
                  "Memorial Day",
                  null,
                  "26.072330097087377",
                  "24.0",
                  "9999.0"
                ]
              ],
              "schema": [
                "summary",
                "country_code",
                "vendorID",
                "passengerCount",
                "tripDistance",
                "tipAmount",
                "totalAmount",
                "tripType",
                "puYear",
                "puMonth",
                "month_num",
                "day_of_month",
                "day_of_week",
                "hour_of_day",
                "countryOrRegion",
                "holidayName",
                "normalizeHolidayName",
                "avg_snowDepth",
                "avg_temperature",
                "max_precipTime",
                "max_precipDepth"
              ]
            }
          },
          "execution_count": 21,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Run the describe() function on the new dataframe to see summary statistics for each field.\n",
        "\n",
        "display(nyc_taxi_holiday_weather_df.describe())"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The summary statistics shows that the totalAmount field has negative values, which don't make sense in the context.\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Remove invalid rows with less than 0 taxi fare or tip\n",
        "final_df = nyc_taxi_holiday_weather_df.filter(nyc_taxi_holiday_weather_df.tipAmount > 0)\\\n",
        "                                      .filter(nyc_taxi_holiday_weather_df.totalAmount > 0)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning up the existing Database\n",
        "\n",
        "First we need to drop the tables since Spark requires that a database is empty before we can drop the Database.\n",
        "\n",
        "Then we recreate the database and set the default database context to it."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "DataFrame[]"
          },
          "execution_count": 23,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "spark.sql(\"DROP TABLE IF EXISTS NYCTaxi.nyc_taxi_holiday_weather\"); "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "DataFrame[]"
          },
          "execution_count": 24,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "spark.sql(\"DROP DATABASE IF EXISTS NYCTaxi\"); \n",
        "spark.sql(\"CREATE DATABASE NYCTaxi\"); \n",
        "spark.sql(\"USE NYCTaxi\");"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a new table\n",
        "We create a nyc_taxi_holiday_weather table from the nyc_taxi_holiday_weather dataframe.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------+\n|count(1)|\n+--------+\n|  337444|\n+--------+"
          },
          "execution_count": 25,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "final_df.write.saveAsTable(\"nyc_taxi_holiday_weather\");\n",
        "spark.sql(\"SELECT COUNT(*) FROM nyc_taxi_holiday_weather\").show();"
      ],
      "attachments": {}
    }
  ]
}