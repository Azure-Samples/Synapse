{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "c8a7fefc-97ec-40a9-853d-7b66c52ab3cd",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Population and safety"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "28505d38-ad93-4f58-b6ab-f1fbf2c0fce6",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Chicago Safety Data\n",
                "\n",
                "311 service requests from the city of Chicago, including historical sanitation code complaints, pot holes reported, and street light issues"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"34e516fb-9e8a-40ac-9778-42165e9aac3f\",\"activityId\":\"a32ae349-2404-45a3-bc20-59ce289cb0f7\",\"applicationId\":\"application_1665739271442_0001\",\"jobGroupId\":\"10\",\"advices\":{\"warn\":1}}"
                },
                "azdata_cell_guid": "5acbc6f3-3372-4c18-86cd-e7c24ac2a4c0",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"citydatacontainer\"\n",
                "blob_relative_path = \"Safety/Release/city=Chicago\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"chicago_safety_data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "e3bc971c-7ec5-49e9-a9d4-d3ea4b3992d1",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## San Francisco Safety Data\n",
                "\n",
                "Fire department calls for service and 311 cases in San Francisco."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"34e516fb-9e8a-40ac-9778-42165e9aac3f\",\"activityId\":\"a32ae349-2404-45a3-bc20-59ce289cb0f7\",\"applicationId\":\"application_1665739271442_0001\",\"jobGroupId\":\"11\",\"advices\":{\"warn\":1}}"
                },
                "azdata_cell_guid": "abcde903-5f73-4e0e-866e-614b489ca596",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:49:12.7036298Z",
                            "execution_start_time": "2022-10-14T09:48:48.3774884Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.821079Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:49:10.916GMT",
                                        "dataRead": 4535,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 11:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=SanFrancisco\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/san_francisco_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "11",
                                        "jobId": 50,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            78,
                                            79,
                                            80
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:10.833GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:10.802GMT",
                                        "dataRead": 7700,
                                        "dataWritten": 4535,
                                        "description": "Delta: Job group for statement 11:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=SanFrancisco\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/san_francisco_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "11",
                                        "jobId": 49,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 62,
                                        "stageIds": [
                                            76,
                                            77
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:09.165GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:09.009GMT",
                                        "dataRead": 10638,
                                        "dataWritten": 7700,
                                        "description": "Delta: Job group for statement 11:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=SanFrancisco\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/san_francisco_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "11",
                                        "jobId": 48,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 24,
                                        "stageIds": [
                                            75
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:08.685GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:07.821GMT",
                                        "dataRead": 450256351,
                                        "dataWritten": 449167173,
                                        "description": "Job group for statement 11:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=SanFrancisco\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/san_francisco_safety_data\")",
                                        "jobGroup": "11",
                                        "jobId": 47,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 112,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 112,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 112,
                                        "rowCount": 23404790,
                                        "stageIds": [
                                            74
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:48:50.081GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:48:49.684GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 11:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=SanFrancisco\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/san_francisco_safety_data\")",
                                        "jobGroup": "11",
                                        "jobId": 46,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            73
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:48:49.208GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 11
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 11, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://citydatacontainer@azureopendatastorage.blob.core.windows.net/Safety/Release/city=SanFrancisco\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"citydatacontainer\"\n",
                "blob_relative_path = \"Safety/Release/city=SanFrancisco\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"san_francisco_safety_data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "338dafdd-83ae-4d56-b566-02682dfc79df",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Seattle Safety Data\n",
                "\n",
                "Seattle Fire Department 911 dispatches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "azdata_cell_guid": "a7e885b8-1183-4f36-9d32-faaa9af08f41",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:49:23.4276318Z",
                            "execution_start_time": "2022-10-14T09:49:12.7983424Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.822568Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:49:22.399GMT",
                                        "dataRead": 4537,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 12:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Seattle\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/seattle_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "12",
                                        "jobId": 55,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            88,
                                            86,
                                            87
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:22.243GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:22.221GMT",
                                        "dataRead": 7075,
                                        "dataWritten": 4537,
                                        "description": "Delta: Job group for statement 12:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Seattle\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/seattle_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "12",
                                        "jobId": 54,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 61,
                                        "stageIds": [
                                            84,
                                            85
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:21.951GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:21.810GMT",
                                        "dataRead": 8843,
                                        "dataWritten": 7075,
                                        "description": "Delta: Job group for statement 12:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Seattle\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/seattle_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "12",
                                        "jobId": 53,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 22,
                                        "stageIds": [
                                            83
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:21.245GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:20.320GMT",
                                        "dataRead": 47516904,
                                        "dataWritten": 46753295,
                                        "description": "Job group for statement 12:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Seattle\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/seattle_safety_data\")",
                                        "jobGroup": "12",
                                        "jobId": 52,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 16,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 16,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 16,
                                        "rowCount": 3449820,
                                        "stageIds": [
                                            82
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:14.505GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:13.996GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 12:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Seattle\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/seattle_safety_data\")",
                                        "jobGroup": "12",
                                        "jobId": 51,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            81
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:13.624GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 12
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 12, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://citydatacontainer@azureopendatastorage.blob.core.windows.net/Safety/Release/city=Seattle\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"citydatacontainer\"\n",
                "blob_relative_path = \"Safety/Release/city=Seattle\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"seattle_safety_data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b00916dc-e3d6-4a4e-85b3-5e8509e06668",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Boston Safety Data\n",
                "\n",
                "311 calls reported to the city of Boston."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "azdata_cell_guid": "c33914fd-7d86-48a8-b946-f64599a1d51a",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:49:38.6072163Z",
                            "execution_start_time": "2022-10-14T09:49:23.5208275Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.8245363Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:49:37.291GMT",
                                        "dataRead": 4537,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 13:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Boston\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/boston_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "13",
                                        "jobId": 60,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            96,
                                            94,
                                            95
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:37.088GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:37.058GMT",
                                        "dataRead": 7813,
                                        "dataWritten": 4537,
                                        "description": "Delta: Job group for statement 13:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Boston\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/boston_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "13",
                                        "jobId": 59,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 61,
                                        "stageIds": [
                                            93,
                                            92
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:35.951GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:35.533GMT",
                                        "dataRead": 10018,
                                        "dataWritten": 7813,
                                        "description": "Delta: Job group for statement 13:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Boston\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/boston_safety_data\"): Compute snapshot for version: 0",
                                        "jobGroup": "13",
                                        "jobId": 58,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 22,
                                        "stageIds": [
                                            91
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:34.963GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:34.025GMT",
                                        "dataRead": 64507999,
                                        "dataWritten": 64937763,
                                        "description": "Job group for statement 13:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Boston\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/boston_safety_data\")",
                                        "jobGroup": "13",
                                        "jobId": 57,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 16,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 16,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 16,
                                        "rowCount": 4184042,
                                        "stageIds": [
                                            90
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:24.952GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:24.489GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 13:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"citydatacontainer\"\nblob_relative_path = \"Safety/Release/city=Boston\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/boston_safety_data\")",
                                        "jobGroup": "13",
                                        "jobId": 56,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            89
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:24.134GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 13
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 13, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://citydatacontainer@azureopendatastorage.blob.core.windows.net/Safety/Release/city=Boston\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"citydatacontainer\"\n",
                "blob_relative_path = \"Safety/Release/city=Boston\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"boston_safety_data\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "66f161ef-fa48-48c5-a3f7-d656542ac5d5",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## US Population by ZIP code\n",
                "\n",
                "US population by gender and race for each US ZIP code sourced from 2000 and 2010 Decennial Census.\n",
                "\n",
                "This dataset is sourced from United States Census Bureaus Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "azdata_cell_guid": "376194fe-3fcc-44b1-90b1-229c771582bc",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:49:53.6537429Z",
                            "execution_start_time": "2022-10-14T09:49:38.6898755Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.826055Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:49:51.773GMT",
                                        "dataRead": 4646,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 14:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_zip/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_zip\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "14",
                                        "jobId": 65,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            102,
                                            103,
                                            104
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:51.693GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:51.663GMT",
                                        "dataRead": 13707,
                                        "dataWritten": 4646,
                                        "description": "Delta: Job group for statement 14:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_zip/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_zip\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "14",
                                        "jobId": 64,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 76,
                                        "stageIds": [
                                            100,
                                            101
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:51.461GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:51.311GMT",
                                        "dataRead": 17235,
                                        "dataWritten": 13707,
                                        "description": "Delta: Job group for statement 14:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_zip/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_zip\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "14",
                                        "jobId": 63,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 52,
                                        "stageIds": [
                                            99
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:50.915GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:49.786GMT",
                                        "dataRead": 124852739,
                                        "dataWritten": 62019478,
                                        "description": "Job group for statement 14:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_zip/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_zip\")\n",
                                        "jobGroup": "14",
                                        "jobId": 62,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 24,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 24,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 24,
                                        "rowCount": 38154240,
                                        "stageIds": [
                                            98
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:40.560GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:40.066GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 14:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_zip/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_zip\")\n",
                                        "jobGroup": "14",
                                        "jobId": 61,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            97
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:39.406GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 14
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 14, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://censusdatacontainer@azureopendatastorage.blob.core.windows.net/release/us_population_zip/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"censusdatacontainer\"\n",
                "blob_relative_path = \"release/us_population_zip/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_population_zip\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f89265a3-b971-4b90-b0c0-6f7b05ae7c86",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## US Population by County\n",
                "\n",
                "US population by gender and race for each US county sourced from 2000 and 2010 Decennial Census.\n",
                "\n",
                "This dataset is sourced from United States Census Bureaus Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "azdata_cell_guid": "730b6e51-2e17-47af-850e-56bcc172c020",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:50:13.3706252Z",
                            "execution_start_time": "2022-10-14T09:49:53.7383386Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.8277708Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:50:11.964GMT",
                                        "dataRead": 4452,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 15:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_county/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_county\"): Compute snapshot for version: 0",
                                        "jobGroup": "15",
                                        "jobId": 70,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            111,
                                            112,
                                            110
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:11.863GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:11.837GMT",
                                        "dataRead": 3774,
                                        "dataWritten": 4452,
                                        "description": "Delta: Job group for statement 15:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_county/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_county\"): Compute snapshot for version: 0",
                                        "jobGroup": "15",
                                        "jobId": 69,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 57,
                                        "stageIds": [
                                            108,
                                            109
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:10.104GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:09.965GMT",
                                        "dataRead": 4594,
                                        "dataWritten": 3774,
                                        "description": "Delta: Job group for statement 15:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_county/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_county\"): Compute snapshot for version: 0",
                                        "jobGroup": "15",
                                        "jobId": 68,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 14,
                                        "stageIds": [
                                            107
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:09.641GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:05.049GMT",
                                        "dataRead": 15662834,
                                        "dataWritten": 11782965,
                                        "description": "Job group for statement 15:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_county/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_county\")",
                                        "jobGroup": "15",
                                        "jobId": 67,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 4,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 4,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 4,
                                        "rowCount": 7329024,
                                        "stageIds": [
                                            106
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:56.304GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:49:55.809GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 15:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"censusdatacontainer\"\nblob_relative_path = \"release/us_population_county/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_population_county\")",
                                        "jobGroup": "15",
                                        "jobId": 66,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            105
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:49:54.528GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 15
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 15, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://censusdatacontainer@azureopendatastorage.blob.core.windows.net/release/us_population_county/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"censusdatacontainer\"\n",
                "blob_relative_path = \"release/us_population_county/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_population_county\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "06f9bc26-79a6-45ce-b4a9-b906f4870bc1",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## US Labor Force Statistics\n",
                "\n",
                "Labor Force Statistics labor force, labor force participation rates, and the civilian noninstitutional population by age, gender, race, and ethnic groups. in the United States.\n",
                "\n",
                "This dataset is sourced from Current Employment Statistics - CES (National) data published by US Bureau of Labor Statistics (BLS). Review Linking and Copyright Information and Important Web Site Notices for the terms and conditions related to the use this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "azdata_cell_guid": "3636872f-f144-4275-988c-419df683700c",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:50:35.5825896Z",
                            "execution_start_time": "2022-10-14T09:50:13.4636768Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.8294855Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:50:34.390GMT",
                                        "dataRead": 4760,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 16:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_labor_force_statistics\"): Compute snapshot for version: 0",
                                        "jobGroup": "16",
                                        "jobId": 75,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            118,
                                            119,
                                            120
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:33.972GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:33.949GMT",
                                        "dataRead": 8602,
                                        "dataWritten": 4760,
                                        "description": "Delta: Job group for statement 16:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_labor_force_statistics\"): Compute snapshot for version: 0",
                                        "jobGroup": "16",
                                        "jobId": 74,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 61,
                                        "stageIds": [
                                            117,
                                            116
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:33.704GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:33.556GMT",
                                        "dataRead": 20192,
                                        "dataWritten": 8602,
                                        "description": "Delta: Job group for statement 16:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_labor_force_statistics\"): Compute snapshot for version: 0",
                                        "jobGroup": "16",
                                        "jobId": 73,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 22,
                                        "stageIds": [
                                            115
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:33.288GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:31.782GMT",
                                        "dataRead": 147285600,
                                        "dataWritten": 19013003,
                                        "description": "Job group for statement 16:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_labor_force_statistics\")",
                                        "jobGroup": "16",
                                        "jobId": 72,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 10,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 10,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 10,
                                        "rowCount": 13165452,
                                        "stageIds": [
                                            114
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:15.679GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:15.148GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 16:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/us_labor_force_statistics\")",
                                        "jobGroup": "16",
                                        "jobId": 71,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            113
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:14.049GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 16
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 16, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/lfs/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"lfs/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "f0ffa504-114c-420b-bdb5-b289206a653c",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US National Employment Hours and Earnings**\n",
                "\n",
                "The Current Employment Statistics (CES) program produces detailed industry estimates of nonfarm employment, hours, and earnings of workers on payrolls in the United States."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "azdata_cell_guid": "cf3da585-2be1-4b6e-859b-45989c82e6e8",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:51:00.8416606Z",
                            "execution_start_time": "2022-10-14T09:50:41.1116065Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:40:41.5640112Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:50:59.306GMT",
                                        "dataRead": 4559,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 18:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_national\"): Compute snapshot for version: 0",
                                        "jobGroup": "18",
                                        "jobId": 85,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            135,
                                            136,
                                            134
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:59.219GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:59.196GMT",
                                        "dataRead": 8302,
                                        "dataWritten": 4559,
                                        "description": "Delta: Job group for statement 18:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_national\"): Compute snapshot for version: 0",
                                        "jobGroup": "18",
                                        "jobId": 84,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 61,
                                        "stageIds": [
                                            132,
                                            133
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:58.999GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:58.849GMT",
                                        "dataRead": 12098,
                                        "dataWritten": 8302,
                                        "description": "Delta: Job group for statement 18:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_national\"): Compute snapshot for version: 0",
                                        "jobGroup": "18",
                                        "jobId": 83,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 22,
                                        "stageIds": [
                                            131
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:57.263GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:56.299GMT",
                                        "dataRead": 96058924,
                                        "dataWritten": 22376463,
                                        "description": "Job group for statement 18:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_national\")",
                                        "jobGroup": "18",
                                        "jobId": 82,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 9,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 9,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 9,
                                        "rowCount": 15284820,
                                        "stageIds": [
                                            130
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:42.816GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:42.270GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 18:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_national\")",
                                        "jobGroup": "18",
                                        "jobId": 81,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            129
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:41.597GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 18
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 18, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ehe_national/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"ehe_national/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "5b199b98-3370-46dd-bc21-0b3c7c9e5183",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US State Employment Hours and Earnings**\n",
                "\n",
                "The Current Employment Statistics (CES) program produces detailed industry estimates of nonfarm employment, hours, and earnings of workers on payrolls in the United States."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "azdata_cell_guid": "3d934f4c-287a-470e-a84b-baa9cb888433",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:51:34.3913619Z",
                            "execution_start_time": "2022-10-14T09:51:00.9404703Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:44:42.6511918Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:51:33.567GMT",
                                        "dataRead": 4552,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 19:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_state\"): Compute snapshot for version: 0",
                                        "jobGroup": "19",
                                        "jobId": 90,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            143,
                                            144,
                                            142
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:33.473GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:33.439GMT",
                                        "dataRead": 7120,
                                        "dataWritten": 4552,
                                        "description": "Delta: Job group for statement 19:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_state\"): Compute snapshot for version: 0",
                                        "jobGroup": "19",
                                        "jobId": 89,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 59,
                                        "stageIds": [
                                            140,
                                            141
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:33.263GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:33.121GMT",
                                        "dataRead": 10679,
                                        "dataWritten": 7120,
                                        "description": "Delta: Job group for statement 19:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_state\"): Compute snapshot for version: 0",
                                        "jobGroup": "19",
                                        "jobId": 88,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 18,
                                        "stageIds": [
                                            139
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:32.764GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:31.854GMT",
                                        "dataRead": 60122382,
                                        "dataWritten": 17249978,
                                        "description": "Job group for statement 19:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_state\")",
                                        "jobGroup": "19",
                                        "jobId": 87,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 7,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 7,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 7,
                                        "rowCount": 16178798,
                                        "stageIds": [
                                            138
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:02.923GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:02.355GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 19:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/ehe_state\")",
                                        "jobGroup": "19",
                                        "jobId": 86,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            137
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:01.498GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 19
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 19, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ehe_state/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"ehe_state/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "1c6b499e-3759-46ee-b4f2-84cfb1c4b1e4",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US Local Area Unemployment Statistics Article**\n",
                "\n",
                "The Local Area Unemployment Statistics (LAUS) program produces monthly and annual employment, unemployment, and labor force data for Census regions and divisions, States, counties, metropolitan areas, and many cities in the United States."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {
                "azdata_cell_guid": "9c96e3b0-cfe5-467c-a80d-7e92e3c167c2",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:51:59.4120219Z",
                            "execution_start_time": "2022-10-14T09:51:34.4816248Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:47:23.8064112Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:51:57.820GMT",
                                        "dataRead": 4635,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 20:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/US Local Area Unemployment Statistics\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "20",
                                        "jobId": 95,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            150,
                                            151,
                                            152
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:57.639GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:57.610GMT",
                                        "dataRead": 13922,
                                        "dataWritten": 4635,
                                        "description": "Delta: Job group for statement 20:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/US Local Area Unemployment Statistics\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "20",
                                        "jobId": 94,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 67,
                                        "stageIds": [
                                            148,
                                            149
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:57.414GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:57.254GMT",
                                        "dataRead": 20921,
                                        "dataWritten": 13922,
                                        "description": "Delta: Job group for statement 20:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/US Local Area Unemployment Statistics\")\n: Compute snapshot for version: 0",
                                        "jobGroup": "20",
                                        "jobId": 93,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 34,
                                        "stageIds": [
                                            147
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:56.587GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:55.621GMT",
                                        "dataRead": 302520864,
                                        "dataWritten": 36380779,
                                        "description": "Job group for statement 20:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/US Local Area Unemployment Statistics\")\n",
                                        "jobGroup": "20",
                                        "jobId": 92,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 16,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 16,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 16,
                                        "rowCount": 24578104,
                                        "stageIds": [
                                            146
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:37.730GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:51:37.287GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 20:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/US Local Area Unemployment Statistics\")\n",
                                        "jobGroup": "20",
                                        "jobId": 91,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            145
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:51:35.017GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 20
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 20, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/laus/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"laus/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"US Local Area Unemployment Statistics\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "223f2879-f884-4834-8eb3-8f3970e78efb",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US Consumer Price Index**\n",
                "\n",
                "The Consumer Price Index (CPI) is a measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {
                "azdata_cell_guid": "ce112714-f145-4295-9dce-cf1a4a105892",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:52:28.4048886Z",
                            "execution_start_time": "2022-10-14T09:51:59.496246Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:49:19.5628326Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:52:28.044GMT",
                                        "dataRead": 4508,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 21:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"cpi/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsConsumerPriceIndex\"): Compute snapshot for version: 0",
                                        "jobGroup": "21",
                                        "jobId": 100,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            158,
                                            159,
                                            160
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:27.967GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:27.941GMT",
                                        "dataRead": 5644,
                                        "dataWritten": 4508,
                                        "description": "Delta: Job group for statement 21:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"cpi/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsConsumerPriceIndex\"): Compute snapshot for version: 0",
                                        "jobGroup": "21",
                                        "jobId": 99,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 58,
                                        "stageIds": [
                                            157,
                                            156
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:27.757GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:27.611GMT",
                                        "dataRead": 7452,
                                        "dataWritten": 5644,
                                        "description": "Delta: Job group for statement 21:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"cpi/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsConsumerPriceIndex\"): Compute snapshot for version: 0",
                                        "jobGroup": "21",
                                        "jobId": 98,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 16,
                                        "stageIds": [
                                            155
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:27.050GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:26.143GMT",
                                        "dataRead": 58295320,
                                        "dataWritten": 17549870,
                                        "description": "Job group for statement 21:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"cpi/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsConsumerPriceIndex\")",
                                        "jobGroup": "21",
                                        "jobId": 97,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 6,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 6,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 6,
                                        "rowCount": 23336808,
                                        "stageIds": [
                                            154
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:01.510GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:01.069GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 21:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"cpi/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsConsumerPriceIndex\")",
                                        "jobGroup": "21",
                                        "jobId": 96,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            153
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:00.102GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 21
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 21, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/cpi/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"cpi/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"Us Consumer Price Index\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "06cc940d-2600-4a55-8916-bb45775ef846",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US Consumer Price Index - industry**\n",
                "\n",
                "The Producer Price Index (PPI) is a measure of average change over time in the selling prices received by domestic producers for their output. The prices included in the PPI are from the first commercial transaction for products and services covered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {
                "azdata_cell_guid": "9644bb6f-be92-4fe8-ad16-f75e75c36c6c",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:52:41.5310777Z",
                            "execution_start_time": "2022-10-14T09:52:28.4909696Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:51:37.0054428Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:52:39.626GMT",
                                        "dataRead": 4444,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 22:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_industry/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexIndustry\"): Compute snapshot for version: 0",
                                        "jobGroup": "22",
                                        "jobId": 105,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            168,
                                            166,
                                            167
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:39.537GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:39.504GMT",
                                        "dataRead": 1894,
                                        "dataWritten": 4444,
                                        "description": "Delta: Job group for statement 22:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_industry/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexIndustry\"): Compute snapshot for version: 0",
                                        "jobGroup": "22",
                                        "jobId": 104,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 54,
                                        "stageIds": [
                                            165,
                                            164
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:39.341GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:39.185GMT",
                                        "dataRead": 2670,
                                        "dataWritten": 1894,
                                        "description": "Delta: Job group for statement 22:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_industry/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexIndustry\"): Compute snapshot for version: 0",
                                        "jobGroup": "22",
                                        "jobId": 103,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 8,
                                        "stageIds": [
                                            163
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:38.159GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:37.199GMT",
                                        "dataRead": 4461414,
                                        "dataWritten": 2152464,
                                        "description": "Job group for statement 22:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_industry/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexIndustry\")",
                                        "jobGroup": "22",
                                        "jobId": 102,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 1905234,
                                        "stageIds": [
                                            162
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:29.977GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:52:29.582GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 22:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_industry/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexIndustry\")",
                                        "jobGroup": "22",
                                        "jobId": 101,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            161
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:52:29.082GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 22
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 22, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ppi_industry/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"ppi_industry/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"Us Producer Price Index Industry\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "4372f553-7730-48c6-843d-ce8087b3b627",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "**US Producer Price Index - Commodities**\n",
                "\n",
                "The Producer Price Index (PPI) is a measure of average change over time in the selling prices received by domestic producers for their output. The prices included in the PPI are from the first commercial transaction for products and services covered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {
                "azdata_cell_guid": "095b1bd7-fbcc-42a2-aca6-28ac0b740934",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": null,
                            "execution_start_time": "2022-10-14T09:53:32.0827783Z",
                            "livy_statement_state": "running",
                            "queued_time": "2022-10-14T09:53:31.9724573Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:53:56.184GMT",
                                        "dataRead": 20610874,
                                        "dataWritten": 4154259,
                                        "description": "Job group for statement 23:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_commodity/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexCommodities\")",
                                        "jobGroup": "23",
                                        "jobId": 107,
                                        "killedTasksSummary": {},
                                        "name": "save at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 3,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 3,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 3,
                                        "rowCount": 13716454,
                                        "stageIds": [
                                            170
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:53:33.832GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:53:33.327GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 23:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ppi_commodity/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/UsProducerPriceIndexCommodities\")",
                                        "jobGroup": "23",
                                        "jobId": 106,
                                        "killedTasksSummary": {},
                                        "name": "parquet at <unknown>:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            169
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:53:32.629GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 2,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "submitted",
                            "statement_id": 23
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 23, Submitted, Running)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ppi_commodity/\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"laborstatisticscontainer\"\n",
                "blob_relative_path = \"ppi_commodity/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"Us Producer Price Index Commodities\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "9aaabcab-7e09-411e-9bea-ca430cc005dc",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Common datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6f46c3c9-51a5-4323-87b6-e9b87dec6662",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Public Holidays\n",
                "\n",
                "Worldwide public holiday data sourced from PyPI holidays package and Wikipedia, covering 38 countries or regions from 1970 to 2099."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "azdata_cell_guid": "f833d2e4-25fe-49da-b283-764dca14ab44",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:50:41.011985Z",
                            "execution_start_time": "2022-10-14T09:50:35.6677238Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.8309855Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:50:40.289GMT",
                                        "dataRead": 4404,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 80,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            126,
                                            127,
                                            128
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:40.153GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:40.126GMT",
                                        "dataRead": 1631,
                                        "dataWritten": 4404,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 79,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 54,
                                        "stageIds": [
                                            125,
                                            124
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:39.964GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:39.819GMT",
                                        "dataRead": 1916,
                                        "dataWritten": 1631,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 78,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 8,
                                        "stageIds": [
                                            123
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:39.508GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:38.693GMT",
                                        "dataRead": 331687,
                                        "dataWritten": 357946,
                                        "description": "Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\")",
                                        "jobGroup": "17",
                                        "jobId": 77,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 139114,
                                        "stageIds": [
                                            122
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:37.060GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:36.666GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\")",
                                        "jobGroup": "17",
                                        "jobId": 76,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            121
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:36.339GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 17
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 17, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://holidaydatacontainer@azureopendatastorage.blob.core.windows.net/Processed\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"holidaydatacontainer\"\n",
                "blob_relative_path = \"Processed\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"public_holidays\")"
            ]
        }
    ],
    "metadata": {
        "kernel_info": {
            "name": "synapse_pyspark"
        },
        "kernelspec": {
            "display_name": "Python 3.10.0 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "notebook_environment": {},
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.livy.synapse.ipythonInterpreter.enabled": "true"
                },
                "enableDebugMode": false,
                "keepAliveTimeout": 30
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {
                "default_lakehouse": "2791fb4b-754c-4583-ac4d-5cd3fed9d9ba",
                "default_lakehouse_name": "DemoLakehouse",
                "known_lakehouses": [
                    {
                        "id": "2791fb4b-754c-4583-ac4d-5cd3fed9d9ba"
                    },
                    {
                        "id": "157150a6-6f05-4580-bf2d-82320104d082"
                    }
                ]
            }
        },
        "vscode": {
            "interpreter": {
                "hash": "ad3ff54c1384f41d27445ebeca1522f5ac631ad8cbbc7eb0df89bf1abd899887"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
