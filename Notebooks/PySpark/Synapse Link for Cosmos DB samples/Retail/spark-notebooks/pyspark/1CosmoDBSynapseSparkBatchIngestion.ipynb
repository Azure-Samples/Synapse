{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python",
      "version": "3.8.5-final"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 32-bit",
      "metadata": {
        "interpreter": {
          "hash": "bb162378bcd0865be49969664200f47400a735e3f46a0ab8d5d80e6092c9afde"
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Batch Data Ingestion for Near real-time sales forecasting leveraging Synapse Link for Azure Cosmos DB\n",
        "\n",
        "## Key Information about this notebook\n",
        "\n",
        "* This notebook is part of the Azure Synapse Link for Azure Cosmos DB analitycal sample notebooks. For more information, click [here](../../../README.md). \n",
        "\n",
        "* It was build for Azure Cosmos DB SQL API but you can, by yourself, customize it for Azure Cosmos DB API for MongoDB. Please read about the analytical store inference schema differences between these 2 APIs [here](https://docs.microsoft.com/azure/cosmos-db/analytical-store-introduction#analytical-schema). \n",
        "\n",
        "* This is a Synapse Notebook and it was created to run in Synapse Analytics workspaces. Please make sure that you followed the pre-reqs of the [README](/README.md) file. After that, please execute the steps below in the same order that they are presented here. \n",
        "\n",
        "* From now on, all operations are case sentitive. Please be careful with everything you need to type.\n",
        "\n",
        "## Predictive Analytics\n",
        "\n",
        "Predictive analytics can help us to study and discover the factors that determine the number of sales that a retail store will have in the future. This notebook scenario is [Microsoft Surface](https://www.microsoft.com/en-us/surface) sales forecasting, with artificially created data. The business challenge is a **distributor that wants to predict how many units are necessary in the local warehouse to supply the stores in the area.**\n",
        "\n",
        "We will use Quantitative Models to forecast future data as a function of past data. They are appropriate to use when past numerical data is available and when it is reasonable to assume that some of the patterns in the data are expected to continue into the future. These methods are usually applied to short or intermediate range decisions. For more information, click [here](https://en.wikipedia.org/wiki/Forecasting).\n",
        "\n",
        "\n",
        "<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/store.PNG\" alt=\"Surface Device\" width=\"75%\"/>\n",
        "\n",
        "## Environment Creation\n",
        "\n",
        "### 1. Using the **Data / Linked** tab of your Synapse workspace, one of the pre-requisites mentioned in the README file, create a **RetailData** folder within the root directory of your storage account. Upload to this folder the csv files that are placed within the folder with the same name of this repo. As you can see in the image below, your new folder in the storage account, a data lake, will be in the second level of the file system structure. \n",
        "\n",
        "**Did you know?**  The Synapse workspace is attached to an [ADLS Gen2 storage account](https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction) and the files placed on the default storage account can be accessed using the relative path as below.\n",
        "\n",
        "<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/upload.PNG\" alt=\"Upload\" width=\"75%\"/>\n",
        "\n",
        "### 2. Using the Azure Portal, go to the **Access Control (IAM)** tab, click on the **+Add** and **Add a role assignment** links and add yourself to the **Contributor** role. This will allow you to create databases and tables within from your Azure Synapse Spark Pool.\n",
        "\n",
        "### 3. Using the Azure Portal, go to [Data Explorer](https://docs.microsoft.com/en-us/azure/cosmos-db/data-explorer) of your the Azure Cosmos DB Account and create a database called **RetailSalesDemoDB**. Change the Throughput to Autoscale and set the limit to 40,000 instead of 400, this will speed-up the loading process of the data, scaling down the database when it is not in use. For more information, click [here](https://docs.microsoft.com/en-us/azure/cosmos-db/provision-throughput-autoscale).\n",
        "\n",
        "### 4. In the same Data Explorer, create 3 **Analytical Store** enabled containers: **StoreDemoGraphics**, **RetailSales**, and **Products**. In the portal interface, the container-id is the container name. Important details:\n",
        "+ Use **/id** as the Partition key for all 3 containers.\n",
        "+ Please make sure that **Analytical store** is set to **On** for all 3 containers. \n",
        "\n",
        "\n",
        "<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/new-cont.PNG\" alt=\"New Containers\" width=\"75%\"/>\n",
        "\n",
        "\n",
        "### 5. In your Azure Synapse workspace, go to the **Manage / Linked Services** tab and create a service called **RetailSalesDemoDB** pointing to the database in Cosmos DB created in item 3 above.\n",
        "\n",
        "<img src=\"https://cosmosnotebooksdata.blob.core.windows.net/notebookdata/ls.PNG\" alt=\"Surface Device\" width=\"100%\"/>"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Now let's load the data into Spark DataFrames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfStoreDemoGraphics = (spark\n",
        "                .read\n",
        "                .csv(\"/RetailData/StoreDemoGraphics.csv\", header=True, inferSchema='true')\n",
        "              )\n",
        "\n",
        "dfRetailSales = (spark\n",
        "                .read\n",
        "                .csv(\"/RetailData/RetailSales.csv\", header=True, inferSchema='true')\n",
        "              )\n",
        "\n",
        "dfProduct = (spark\n",
        "                .read\n",
        "                .csv(\"/RetailData/Products.csv\", header=True, inferSchema='true')\n",
        "              )\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Write the dataframe to the Azure Cosmos DB Collections\n",
        "\n",
        ">**Did you know?** The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store.\n",
        "\n",
        ">**Did you know?** The ingestion to the Azure Cosmos DB collection is always performed through the Transactional store irrespective of whether the Analytical Store is enabled or not."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfStoreDemoGraphics.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
        "            .option(\"spark.cosmos.container\", \"StoreDemoGraphics\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()\n",
        "\n",
        "dfRetailSales.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
        "            .option(\"spark.cosmos.container\", \"RetailSales\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()\n",
        "\n",
        "dfProduct.write\\\n",
        "            .format(\"cosmos.oltp\")\\\n",
        "            .option(\"spark.synapse.linkedService\", \"RetailSalesDemoDB\")\\\n",
        "            .option(\"spark.cosmos.container\", \"Products\")\\\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\n",
        "            .mode('append')\\\n",
        "            .save()     \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Using the Azure Cosmos DB Account portal, go to the Data Explorer and check if the data was loaded.\n",
        "\n",
        "## All Done! Now let's go to the [Forecast Notebook](2SalesForecastingWithAML.ipynb)."
      ]
    }
  ]
}