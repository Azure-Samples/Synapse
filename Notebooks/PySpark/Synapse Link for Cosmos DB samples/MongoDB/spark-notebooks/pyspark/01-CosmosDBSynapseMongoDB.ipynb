{
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with Azure Cosmos DB's API for MongoDB and Synapse Link\n",
        "\n",
        "In this sample we will execute the following tasks:\n",
        "\n",
        "1. Insert a dataset using the traditional MongoDB client.\n",
        "1. Execute aggregation queries against the Analytical Store from the transactional data we inserted.\n",
        "1. Insert another dataset, but this time using the MongoSpark connector.\n",
        "1. Execute aggregation queries again, consolidating both datasets.\n",
        "\n",
        "## Pre-requisites\n",
        "1. Have you created a MongoDB API account in Azure Cosmos DB? If not, go to [Create an account for Azure Cosmos DB's API for MongoDB](https://docs.microsoft.com/azure/cosmos-db/create-cosmosdb-resources-portal#create-an-azure-cosmos-db-account). Be sure to create an account using MongoDB as the API option.\n",
        "1. For your Cosmos DB account, have you enabled Synapse Link? If not, go to [Enable Synapse Link for Azure Cosmos DB accounts](https://docs.microsoft.com/azure/cosmos-db/configure-synapse-link#enable-synapse-link).\n",
        "1. Have you created a Synapse Workspace? If not, go to [Create a Synapse Workspace](https://docs.microsoft.com/azure/synapse-analytics/quickstart-create-workspace).\n",
        "\n",
        "## Create a Cosmos DB collection with Synapse Link\n",
        "1. Create a database named `test`. \n",
        "1. Create a collection named `htap`.\n",
        "    - Make sure you set the `Storage capacity` option to `Fixed` when you create your collection.\n",
        "    - Make sure you set the `Analytical store` option to `On` when you create your collection.\n",
        "\n",
        "## Connect your collection to Synapse\n",
        "1. Go to your Synapse Analytics workspace.\n",
        "1. Create a `Linked Data` connection for your MongoDB API account. \n",
        "    1. Under the `Data` blade, select the + (plus) sign.\n",
        "    1. Select the `Connect to external data` option.\n",
        "    1. Now select the `Azure Cosmos DB (MongoDB API)` option. \n",
        "    1. Enter all the information regarding your specific Azure Cosmos DB account either by using the dropdowns or by entering the connection string. Take note of the name you assigned to your `Linked Data` connection. \n",
        "    - Alternatively, you can also use the connection parameters from your account overview.\n",
        "1. Test the connection by looking for your database accounts in the `Data` blade, and under the `Linked` tab.\n",
        "    - There should be a list that contains all accounts and collections.\n",
        "    - Collections that have an `Analytical Store` enabled will have a distinctive icon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's get the environment ready\n",
        "\n",
        "This environment allows you to install and use any python libraries that you want to run. For this sample, you need to add the following libraries to your Spark pool:\n",
        "\n",
        "```\n",
        "pymongo==2.8.1\n",
        "aenum==2.1.2\n",
        "backports-abc==0.5\n",
        "bson==0.5.10\n",
        "```\n",
        "\n",
        "Learn how to import libraries into your Spark pools in [this article](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-azure-portal-add-libraries). We recommend creating a new pool for this.\n",
        "\n",
        "You can execute the following command to make sure all the libraries are installed correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "MongoSpark",
              "session_id": 11,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-08-19T21:30:36.2458227Z",
              "execution_start_time": "2020-08-19T21:33:21.3955312Z",
              "execution_finish_time": "2020-08-19T21:33:23.4516169Z"
            },
            "text/plain": "StatementMeta(MongoSpark, 11, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "zipp 0.6.0\nzict 1.0.0\nxlwt 1.2.0\nXlsxWriter 0.9.6\nxlrd 1.0.0\nwrapt 1.11.2\nwidgetsnbextension 2.0.0\nwheel 0.30.0\nWerkzeug 0.16.0\nwebsocket-client 0.56.0\nwcwidth 0.1.7\nvega-datasets 0.7.0\nurllib3 1.25.6\nunicodecsv 0.14.1\ntyping-extensions 3.7.4\ntraitlets 4.3.2\ntqdm 4.48.2\ntornado 6.0.3\ntorch 1.3.0\ntoolz 0.10.0\ntestpath 0.3\nterminado 0.6\ntermcolor 1.1.0\ntensorflow 1.14.0\ntensorflow-estimator 1.14.0\ntensorboard 1.14.0\ntblib 1.4.0\ntables 3.3.0\nsympy 1.0\nstatsmodels 0.10.1\nSQLAlchemy 1.1.9\nspyder 3.1.4\nsortedcontainers 2.1.0\nsortedcollections 0.5.3\nsnowballstemmer 1.2.1\nsmart-open 1.8.4\nsklearn-pandas 1.7.0\nskl2onnx 1.4.9\nsix 1.12.0\nsingledispatch 3.4.0.3\nsimplegeneric 0.8.1\nshap 0.34.0\nsetuptools 41.4.0\nSecretStorage 3.1.1\nseaborn 0.9.0\nscipy 1.1.0\nscikit-learn 0.20.3\nscikit-image 0.15.0\ns3transfer 0.2.1\nruamel.yaml 0.15.89\nrope-py3k 0.9.4.post1\nretrying 1.3.3\nResource 0.2.1\nrequests 2.22.0\nrequests-oauthlib 1.2.0\nQtPy 1.2.1\nqtconsole 4.3.0\nQtAwesome 0.4.4\npyzmq 16.0.2\nPyYAML 5.1.2\nPyWavelets 1.0.3\npytz 2019.3\nPython-EasyConfig 0.1.7\npython-dateutil 2.8.0\npytest 3.0.7\npyspark 2.4.4\npyrsistent 0.15.4\npyparsing 2.4.2\npyOpenSSL 19.0.0\npyodbc 4.0.16\npymssql 2.1.4\npymongo 2.8.1\npylint 1.6.4\nPyJWT 1.7.1\nPygments 2.2.0\npygal 2.4.0\npyflakes 2.1.1\npycurl 7.43.0\npyct 0.4.6\npycrypto 2.6.1\npycparser 2.19\npycosat 0.6.2\npycodestyle 2.5.0\npyasn1 0.4.7\npyarrow 0.15.1\npy4j 0.10.7\npy 1.4.33\npy-cpuinfo 5.0.0\nptyprocess 0.5.1\npsutil 5.2.2\nprotobuf 3.10.0\nprompt-toolkit 2.0.10\nportalocker 1.7.1\npmdarima 1.1.1\nply 3.10\nplotly 4.1.1\npip 9.0.1\nPillow 6.2.0\npickleshare 0.7.4\npexpect 4.2.1\npep8 1.7.0\npatsy 0.5.1\npathspec 0.6.0\npathlib2 2.2.1\npartd 1.0.0\nparam 1.9.2\npandocfilters 1.4.1\npandas 0.23.4\npackaging 19.2\nopenpyxl 2.4.7\nonnxruntime 0.4.0\nonnxmltools 1.4.1\nonnxconverter-common 1.5.5\nonnx 1.6.0\nolefile 0.44\nodo 0.5.0\noauthlib 3.1.0\nnumpydoc 0.6.0\nnumpy 1.16.2\nnumexpr 2.6.2\nnumba 0.33.0\nnotebookutils 20200701.5\nnotebook 5.0.0\nnose 1.3.7\nnltk 3.2.3\nnimbusml 1.5.0\nnetworkx 2.3\nndg-httpsclient 0.5.1\nnbformat 4.3.0\nnbconvert 5.1.1\nnavigator-updater 0.1.0\nmultipledispatch 0.4.9\nmultimethods 1.0.0\nmsrestazure 0.6.2\nmsrest 0.6.10\nmsgpack 0.6.2\nmsgpack-python 0.4.8\nmsal 1.4.3\nmsal-extensions 0.1.3\nmpmath 0.19\nmore-itertools 7.2.0\nmmlspark 1.0.0.dev1\nmistune 0.7.4\nmissingno 0.4.2\nmccabe 0.6.1\nmatplotlib 3.1.1\nMarkupSafe 1.1.1\nMarkdown 3.1.1\nlxml 3.7.3\nlocket 0.2.0\nllvmlite 0.18.0\nlightgbm 2.2.3\nliac-arff 2.4.0\nlazy-object-proxy 1.2.2\nkiwisolver 1.1.0\nkeras2onnx 1.5.2\nKeras-Preprocessing 1.1.0\nKeras-Applications 1.0.8\njupyter 1.0.0\njupyter-core 4.3.0\njupyter-console 5.1.0\njupyter-client 5.0.1\nJsonSir 0.0.2\njsonschema 3.1.1\njsonpickle 1.2\nJsonForm 0.0.2\njson-logging-py 0.2\njoblib 0.14.1\njmespath 0.9.4\nJinja2 2.10.3\njeepney 0.4.1\njedi 0.10.2\njdcal 1.3\nitsdangerous 0.24\nisort 4.2.5\nisodate 0.6.0\nipywidgets 6.0.0\nipython 7.8.0\nipython-genutils 0.2.0\nipykernel 4.6.1\ninterpret-core 0.1.21\ninterpret-community 0.10.2\nimportlib-metadata 0.23\nimagesize 0.7.1\nimageio 2.6.1\nidna 2.8\nhyperspace 0.0.1\nhtml5lib 0.999\nHeapDict 1.0.1\nh5py 2.10.0\ngunicorn 19.9.0\ngrpcio 1.24.1\ngreenlet 0.4.12\ngoogle-pasta 0.1.7\ngevent 1.2.1\ngensim 3.8.1\ngast 0.3.2\nfusepy 3.0.1\nfsspec 0.5.2\nFlask 1.0.3\nFlask-Cors 3.0.2\nflake8 3.7.9\nfire 0.2.1\nfastcache 1.0.2\net-xmlfile 1.0.1\nentrypoints 0.3\ndotnetcore2 2.1.14\ndocutils 0.15.2\ndocker 4.1.0\ndistro 1.4.0\ndistributed 1.16.3\ndill 0.3.1.1\ndecorator 4.4.0\ndatashape 0.5.4\ndask 0.14.3\ncytoolz 0.8.2\nCython 0.29.13\ncycler 0.10.0\ncryptography 2.7\ncontextlib2 0.6.0.post1\nconfigparser 3.7.4\ncolorama 0.3.9\nclyent 1.2.2\ncloudpickle 1.2.2\nclick 6.7\nchart-studio 1.0.0\nchardet 3.0.4\ncffi 1.12.3\ncertifi 2019.9.11\nbson 0.5.10\nBottleneck 1.2.1\nbotocore 1.12.247\nboto3 1.9.247\nboto 2.49.0\nbokeh 1.3.4\nbleach 1.5.0\nblaze 0.10.1\nbitarray 0.8.1\nbeautifulsoup4 4.6.0\nbackports.weakref 1.0.post1\nbackports.tempfile 1.0\nbackports.shutil-get-terminal-size 1.0.0\nbackports-abc 0.5\nbackcall 0.2.0\nBabel 2.4.0\nazureml-train 1.6.0\nazureml-train-restclients-hyperdrive 1.6.0\nazureml-train-core 1.6.0\nazureml-train-automl 1.6.0\nazureml-train-automl-runtime 1.6.0\nazureml-train-automl-client 1.6.0.post1\nazureml-telemetry 1.6.0\nazureml-sdk 1.6.0\nazureml-pipeline 1.6.0\nazureml-pipeline-steps 1.6.0\nazureml-pipeline-core 1.6.0\nazureml-opendatasets 1.6.0\nazureml-model-management-sdk 1.0.1b6.post1\nazureml-interpret 1.6.0\nazureml-explain-model 1.6.0\nazureml-defaults 1.6.0\nazureml-dataprep 1.6.3\nazureml-dataprep-native 14.1.0\nazureml-core 1.6.0\nazureml-automl-runtime 1.6.0.post2\nazureml-automl-core 1.6.0\nazure-storage-common 2.1.0\nazure-storage-blob 2.1.0\nazure-mgmt-storage 4.2.0\nazure-mgmt-resource 5.1.0\nazure-mgmt-network 10.2.0\nazure-mgmt-keyvault 2.0.0\nazure-mgmt-containerregistry 2.8.0\nazure-mgmt-authorization 0.60.0\nazure-identity 1.2.0\nazure-graphrbac 0.61.1\nazure-core 1.7.0\nazure-common 1.1.23\nattrs 19.2.0\nastropy 1.3.2\nastroid 1.4.9\nastor 0.8.0\nasn1crypto 1.0.1\napplicationinsights 0.11.9\naltair 3.2.0\nalabaster 0.7.10\naenum 2.1.2\nadal 1.2.2\nabsl-py 0.8.1\nSphinx 1.5.6"
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "outputCollapsed": true
      },
      "source": [
        "import pip #needed to use the pip functions\n",
        "\n",
        "for i in pip.get_installed_distributions(local_only=True):\n",
        "    print(i)\n",
        "\n",
        "# The output might be long... you can collapse it by clicking on the 'Collapse output' option on the upper left corner of the output cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write your database account specific secrets here!\n",
        "\n",
        "We won't tell anybody.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "MongoSpark",
              "session_id": 9,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-08-19T07:19:55.1572228Z",
              "execution_start_time": "2020-08-19T07:19:55.1953802Z",
              "execution_finish_time": "2020-08-19T07:19:57.2244591Z"
            },
            "text/plain": "StatementMeta(MongoSpark, 9, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "DATABASE_ACCOUNT_NAME = '<your Cosmos DB account name>'\n",
        "DATABASE_ACCOUNT_READWRITE_KEY = '<Readable and writable primary or secondary password of your Cosmos DB account>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's initialize the MongoDB client\n",
        "\n",
        "You are only going to need the following parameters from your account overview: \n",
        "- Connection string.\n",
        "- Primary or secondary ready/write key.\n",
        "\n",
        "Remember that we named our database `test` and our collection `htap`.\n",
        "\n",
        "The code snippet below shows how to initialize the `MongoClient` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "MongoSpark",
              "session_id": 9,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-08-19T07:20:48.8352623Z",
              "execution_start_time": "2020-08-19T07:20:48.8750501Z",
              "execution_finish_time": "2020-08-19T07:20:50.9474208Z"
            },
            "text/plain": "StatementMeta(MongoSpark, 9, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pymongo'",
          "traceback": [
            "ModuleNotFoundError: No module named 'pymongo'",
            "Traceback (most recent call last):\n",
            "ModuleNotFoundError: No module named 'pymongo'\n"
          ]
        }
      ],
      "metadata": {},
      "source": [
        "from pymongo import MongoClient\n",
        "from bson import ObjectId # For ObjectId to work\n",
        "\n",
        "client = MongoClient(\"mongodb://{account}.mongo.cosmos.azure.com:10255/?ssl=true&replicaSet=globaldb\".format(account = DATABASE_ACCOUNT_NAME)) # Your own database account endpoint.\n",
        "db = client.test    # Select the database\n",
        "db.authenticate(name=DATABASE_ACCOUNT_NAME,password=DATABASE_ACCOUNT_READWRITE_KEY) # Use your database account name and any of your read/write keys."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inserting data with the MongoClient driver\n",
        "\n",
        "The following sample will generate 500 items based on random data. Each item will contain the following fields:\n",
        "- Item, string\n",
        "- Price, float\n",
        "- Rating, integer\n",
        "- Timestamp, [epoch integer](http://unixtimestamp.50x.eu/about.php)\n",
        "\n",
        "This cell depends on the cell above to create an instance of the connection to the Cosmos DB MongoDB API account.\n",
        "\n",
        "This data will be inserted into the MongoDB store of your database. This emulates the transactional data that an application would generate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "import time\n",
        "\n",
        "orders = db[\"htap\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : time.time()\n",
        "    }\n",
        "    \n",
        "    result=orders.insert(order)\n",
        "\n",
        "print('finished creating 500 orders')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read data from the Analytical Store\n",
        "\n",
        "Now that we have inserted some transactional data, let's read it from the Analytical Store.\n",
        "\n",
        "The data will be automatically transformed into the columnar format, which will make it fast and easy to execute large aggregation queries.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "session_starting",
              "livy_statement_state": null,
              "queued_time": "2020-08-21T08:19:55.7976362Z",
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , SessionStarting, )"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the Analytical Store data into a dataframe\n",
        "# Make sure to run the cell with the secrets to get the DATABASE_ACCOUNT_NAME and the DATABASE_ACCOUNT_READWRITE_KEY variables.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", \"test\")\\\n",
        "    .option(\"spark.cosmos.container\", \"htap\")\\\n",
        "    .load()\n",
        "\n",
        "# Let's find out all the revenue from Pizza orders\n",
        "df.groupBy(df.item.string).sum().show()\n",
        "\n",
        "# df[df.item.string == 'Pizza'].show(10) \n",
        "# df.select(df['item'] == Struct).show(10) \n",
        "# df.select(\"timestamp.float64\").show(10)\n",
        "#df.select(\"timestamp.string\", when(df.timestamp.string != null)).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\r\n",
        "## A quick note about the MongoDB schema in Analytical Store\r\n",
        "\r\n",
        "For MongoDB accounts we make use of a **Full Fidelity Schema**. This is a representation of property names extended with their data types to provide an accurate representation of their values and avoid ambiguity.\r\n",
        "\r\n",
        "This is why, when we called the fields above, we used their datatype as a suffix. Like in the example below:\r\n",
        "\r\n",
        "```\r\n",
        "df.filter((df.item.string == \"Pizza\")).show(10)\r\n",
        "```\r\n",
        "\r\n",
        "Notice how we specified the `string` type after the name of the property. Here is a map of all potential properties and their suffix representations in the Analytical Store:\r\n",
        "\r\n",
        "| Original Data Type     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Suffix    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Example &nbsp;&nbsp;&nbsp;&nbsp; | \r\n",
        "|---------------|----------------|--------|\r\n",
        "| Double        | \".float64\"     |  `24.99`   |\r\n",
        "| Array         | \".array\"       |  `[\"a\", \"b\"]`   |\r\n",
        "| Binary        | \".binary\"      |  `0`   |\r\n",
        "| Boolean       | \".bool\"        |  `True`   |\r\n",
        "| Int32         | \".int32\"       |  `123`   |\r\n",
        "| Int64         | \".int64\"       |  `255486129307`   |\r\n",
        "| Null          | \".null\"        |  `null`   |\r\n",
        "| String        | \".string\"      |  `\"ABC\"`   |\r\n",
        "| Timestamp     | \".timestamp\"   |  `Timestamp(0, 0)`   |\r\n",
        "| DateTime      | \".date\"        |  `ISODate(\"2020-08-21T07:43:07.375Z\")`   |\r\n",
        "| ObjectId      | \".objectId\"    |  `ObjectId(\"5f3f7b59330ec25c132623a2\")`   |\r\n",
        "| Document      | \".object\"      |  `{\"a\": \"a\"}`   |\r\n",
        "\r\n",
        "These types are inferred from the data that is inserted in the transactional store. You can see the schema by executing the following command:\r\n",
        "```\r\n",
        "df.schema\r\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's insert more orders!\n",
        "\n",
        "This time we will use slightly different data. Each item will contain the following fields:\n",
        "- Item, string\n",
        "- Price, float\n",
        "- Rating, integer\n",
        "- Timestamp, [ISO String format](https://en.wikipedia.org/wiki/ISO_8601)\n",
        "\n",
        "Notice how the `Timestamp` field is now in a string format. This will help us understand how the different data fields can be read based on their data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from random import randint\n",
        "from time import strftime\n",
        "\n",
        "orders = db[\"htap\"]\n",
        "\n",
        "items = ['Pizza','Sandwich','Soup', 'Salad', 'Tacos']\n",
        "prices = [2.99, 3.49, 5.49, 12.99, 54.49]\n",
        "\n",
        "for x in range(1, 501):\n",
        "    order = {\n",
        "        'item' : items[randint(0, (len(items)-1))],\n",
        "        'price' : prices[randint(0, (len(prices)-1))],\n",
        "        'rating' : randint(1, 5),\n",
        "        'timestamp' : strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "    \n",
        "    result=orders.insert(order)\n",
        "\n",
        "print('finished creating 500 orders')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let's read that data again!\n",
        "\n",
        "This time, we will be reading the ISO string dates separately by specifying the `timestamp.string` parameter.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "MongoSpark",
              "session_id": 9,
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2020-08-19T08:14:33.1214303Z",
              "execution_start_time": "2020-08-19T08:14:33.168174Z",
              "execution_finish_time": "2020-08-19T08:14:37.245524Z"
            },
            "text/plain": "StatementMeta(MongoSpark, 9, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "execution_count": 26,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------------+----------+--------------------+--------------------+--------------+----------+-------+------+--------------------+---+--------------+\n|                _rid|       _ts|                  id|               _etag|           _id|      item|  price|rating|           timestamp| pk| _partitionKey|\n+--------------------+----------+--------------------+--------------------+--------------+----------+-------+------+--------------------+---+--------------+\n|c8dVAMNlPsb1AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003909-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Tacos]|[12.99]|   [1]|[, 2020-08-18 23:...|[3]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb2AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003a09-0000-08...|[_<`��]\u001b\u001aRUj�]|[Sandwich]|[54.49]|   [5]|[, 2020-08-18 23:...|[1]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb3AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003b09-0000-08...|[_<`��]\u001b\u001aRUj�]|    [Soup]| [5.49]|   [4]|[, 2020-08-18 23:...|[4]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb4AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003c09-0000-08...|[_<`��]\u001b\u001aRUj�]|    [Soup]|[12.99]|   [1]|[, 2020-08-18 23:...|[4]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb5AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003d09-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Tacos]|[12.99]|   [2]|[, 2020-08-18 23:...|[2]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb6AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003e09-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Pizza]|[12.99]|   [3]|[, 2020-08-18 23:...|[2]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb7AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00003f09-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Salad]|[54.49]|   [5]|[, 2020-08-18 23:...|[2]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb8AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00004009-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Salad]| [3.49]|   [1]|[, 2020-08-18 23:...|[5]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb9AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00004109-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Salad]|[54.49]|   [1]|[, 2020-08-18 23:...|[5]|[c8dVAMNlPsY=]|\n|c8dVAMNlPsb+AQAAA...|1597792391|NWYzYzYwODdmYjVkM...|\"00004209-0000-08...|[_<`��]\u001b\u001aRUj�]|   [Salad]|[54.49]|   [1]|[, 2020-08-18 23:...|[5]|[c8dVAMNlPsY=]|\n+--------------------+----------+--------------------+--------------------+--------------+----------+-------+------+--------------------+---+--------------+\nonly showing top 10 rows"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Load the Analytical Store data into a dataframe\n",
        "# Make sure to run the cell with the secrets to get the DATABASE_ACCOUNT_NAME and the DATABASE_ACCOUNT_READWRITE_KEY variables.\n",
        "df = spark.read.format(\"cosmos.olap\")\\\n",
        "    .option(\"spark.cosmos.accountEndpoint\", \"https://{account}.documents.azure.com:443/\".format(account = DATABASE_ACCOUNT_NAME))\\\n",
        "    .option(\"spark.cosmos.accountKey\", DATABASE_ACCOUNT_READWRITE_KEY)\\\n",
        "    .option(\"spark.cosmos.database\", \"test\")\\\n",
        "    .option(\"spark.cosmos.container\", \"htap\")\\\n",
        "    .load()\n",
        "\n",
        "# Let's find out all the revenue from Pizza orders\n",
        "df.filter( (df.timestamp.string != \"\")).show(10)"
      ]
    }
  ]
}