{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Streaming ingestion into Azure Cosmos DB collection using Structured Streaming\n",
        "\n",
        "In this notebook, we'll \n",
        "\n",
        "1. Simulate streaming data generation using Rate streaming source\n",
        "2. Format the stream dataframe as per the IoTSignals schema\n",
        "3. Write the streaming dataframe to the Azure Cosmos DB collection\n",
        "\n",
        ">**Did you know?** Azure Cosmos DB is a great fit for IoT predictive maintenance and anomaly detection use cases. [Click here](https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link-use-cases#iot-predictive-maintenance) to learn more about an IoT architecture leveraging HTAP capabilities of Azure Synapse Link for Azure Cosmos DB.\n",
        "\n",
        ">**Did you know?**  [Azure Synapse Link for Azure Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link) is a hybrid transactional and analytical processing (HTAP) capability that enables you to run near real-time analytics over operational data in Azure Cosmos DB.\n",
        "&nbsp;\n",
        "\n",
        ">**Did you know?**  [Azure Cosmos DB analytical store](https://docs.microsoft.com/en-us/azure/cosmos-db/analytical-store-introduction?branch=release-build-cosmosdb) is a fully isolated column store for enabling large scale analytics against operational data in your Azure Cosmos DB, without any impact to your transactional workloads.\n",
        "&nbsp;"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Simulate streaming data generation using Rate streaming source\n",
        "* The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as [Azure Event Hubs](https://azure.microsoft.com/en-us/services/event-hubs/) and [Apache Kafka](https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction).\n",
        "\n",
        "* [Click here](https://github.com/Azure-Samples/streaming-at-scale) to learn more about the possible ways to implement an end-to-end streaming solution using a choice of different Azure technologies.\n",
        "\n",
        ">**Did you know?**  The Rate streaming source generates data at the specified number of rows per second and each output row contains a timestamp and value."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "dfStream = (spark\n",
        "                .readStream\n",
        "                .format(\"rate\")\n",
        "                .option(\"rowsPerSecond\", 10)\n",
        "                .load()\n",
        "            )"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Format the stream dataframe as per the IoTSignals schema\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import StringType\n",
        "import uuid\n",
        "\n",
        "numberOfDevices = 10\n",
        "generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\n",
        "              \n",
        "dfIoTSignals = (dfStream\n",
        "                    .withColumn(\"id\", generate_uuid())\n",
        "                    .withColumn(\"dateTime\", df[\"timestamp\"].cast(StringType()))\n",
        "                    .withColumn(\"deviceId\", F.concat(F.lit(\"device-id-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)))\n",
        "                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\n",
        "                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\n",
        "                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\n",
        "                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.9 THEN value * 2 WHEN rand() < 0.1 THEN value div 2 ELSE value END\"))\n",
        "                    .drop(\"timestamp\")\n",
        "                )"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Stream writes to the Azure Cosmos DB Collection\n",
        ">**Did you know?** The \"cosmos.oltp\" is the Spark format that enables connection to the Cosmos DB Transactional store.\n",
        "\n",
        ">**Did you know?** The ingestion to the Cosmos DB collection is always performed through the Transactional store irrespective of whether the Analytical Store is enabled or not."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "streamQuery = dfIoTSignals\\\n",
        "                    .writeStream\\\n",
        "                    .format(\"cosmos.oltp\")\\\n",
        "                    .outputMode(\"append\")\\\n",
        "                    .option(\"spark.cosmos.connection.mode\", \"gateway\")\\      \n",
        "                    .option(\"spark.synapse.linkedService\", \"CosmosDBIoTDemo\")\\\n",
        "                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\n",
        "                    .option(\"checkpointLocation\", \"/writeCheckpointDir\")\\\n",
        "                    .start()\n",
        "\n",
        "streamQuery.awaitTermination()"
      ],
      "attachments": {}
    }
  ]
}