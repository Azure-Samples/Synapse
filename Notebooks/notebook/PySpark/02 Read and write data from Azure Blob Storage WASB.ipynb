{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Access data on Azure Storage Blob (WASB) with Synapse Spark\n",
        "\n",
        "You can access data on Azure Storage Blob (WASB) with Synapse Spark via following URL:\n",
        "\n",
        "    wasb[s]://<container_name>@<storage_account_name>.blob.core.windows.net/<path>\n",
        "\n",
        "This notebook provides examples of how to read data from WASB into a Spark context and how to write the output of Spark jobs directly into a WASB location."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\n|countryOrRegion|holidayName              |normalizeHolidayName     |isPaidTimeOff|countryRegionCode|date               |\n+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\n|Czech          |Den české státnosti      |Den české státnosti      |null         |CZ               |2019-09-28 00:00:00|\n|Norway         |Søndag                   |Søndag                   |null         |NO               |2019-09-29 00:00:00|\n|Sweden         |Söndag                   |Söndag                   |null         |SE               |2019-09-29 00:00:00|\n|India          |Gandhi Jayanti           |Gandhi Jayanti           |true         |IN               |2019-10-02 00:00:00|\n|Germany        |Tag der Deutschen Einheit|Tag der Deutschen Einheit|null         |DE               |2019-10-03 00:00:00|\n+---------------+-------------------------+-------------------------+-------------+-----------------+-------------------+\nonly showing top 5 rows"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "hol_df.show(5, truncate = False)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write data to Azure Storage Blob\n",
        "\n",
        "We are going to write the spark dateframe to your Azure Blob Storage (WASB) path using **shared access signature (sas)**. Go to [Azure Portal](https://portal.azure.com/), open your Azure storage blob, select **shared access signature** in the **settings** and generate your sas token. Please make sure to allow contatiner level read and write permission. Fill in the access info for your Azure storage blob in the cell below. \n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Azure storage access info\n",
        "blob_account_name = 'Your blob name' # replace with your blob name\n",
        "blob_container_name = 'Your container name' # replace with your container name\n",
        "blob_relative_path = 'Your relative path' # replace with your relative folder path\n",
        "blob_sas_token = r'Your sas token' # replace with your access key"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "Remote blob path: wasbs://data@samplenbblob.blob.core.windows.net/samplenb/"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Allow SPARK to access from Blob remotely\n",
        "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)\n",
        "print('Remote blob path: ' + wasbs_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save a dataframe as Parquet, JSON or CSV\n",
        "If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
        "\n",
        "Dataframes can be saved in any format, regardless of the input format.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "parquet file path: wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.parquet\njson file path： wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.json\ncsv file path: wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.csv"
          },
          "execution_count": 7,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "parquet_path = wasbs_path + 'holiday.parquet'\n",
        "json_path = wasbs_path + 'holiday.json'\n",
        "csv_path = wasbs_path + 'holiday.csv'\n",
        "print('parquet file path: ' + parquet_path)\n",
        "print('json file path： ' + json_path)\n",
        "print('csv file path: ' + csv_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {},
      "source": [
        "hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
        "hol_df.write.json(json_path, mode = 'overwrite')\n",
        "hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save a dataframe as text files\n",
        "If you have a dataframe that you want ot save as text file, you must first covert it to an RDD and then save that RDD as a text file.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "text file path: wasbs://data@samplenbblob.blob.core.windows.net/samplenb/holiday.txt"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Define the text file path\n",
        "text_path = wasbs_path + 'holiday.txt'\n",
        "print('text file path: ' + text_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<class 'pyspark.rdd.RDD'>"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Covert spark dataframe into RDD \n",
        "hol_RDD = hol_df.rdd\n",
        "type(hol_RDD)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have an RDD, you can convert it to a text file like the following:\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        " # Save RDD as text file\n",
        "hol_RDD.saveAsTextFile(text_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read data from Azure Storage Blob\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from parquet files\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_parquet = spark.read.parquet(parquet_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from JSON files\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_json = spark.read.json(json_path)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a dataframe from CSV files\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_csv = spark.read.csv(csv_path, header = 'true')"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create an RDD from text file\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "text = sc.textFile(text_path)"
      ],
      "attachments": {}
    }
  ]
}