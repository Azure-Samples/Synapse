{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Access Synapse SQL table from Synapse Spark\n",
        "\n",
        "This notebook provides examples of how to read data from Synapse SQL into a Spark context and how to write the output of Spark jobs into an Synapse SQL table.\n",
        "\n",
        "\n",
        "## Limits\n",
        "- Scala is the only supported language by the Spark-SQL connector.\n",
        "- The Spark connector can only read colummns without space in its header in the sql pool.\n",
        "- Columns with time definition in the sql pool not yet supported.\n",
        "- You need to define a container on the workspace's primary or linked storage as the temp data folder.\n",
        "\n",
        "## Pre-requisites\n",
        "You need to be db_owner to read and write in sql pool. Ask your admin to run the following command with your AAD credential:\n",
        "\n",
        "    \n",
        "    EXEC sp_addrolemember 'db_owner', 'AAD@contoso.com'"
        "**Note** - This syntax is not supported by serverless SQL pool in Azure Synapse Analytics."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a sample data\n",
        "\n",
        "Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) from Azure Open datasets as a sample.\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "hol_blob_account_name: String = azureopendatastorage\nhol_blob_container_name: String = holidaydatacontainer\nhol_blob_relative_path: String = Processed\nhol_blob_sas_token: String = \"\"\nhol_wasbs_path: String = wasbs://holidaydatacontainer@azureopendatastorage.blob.core.windows.net/Processed\nhol_df: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 4 more fields]\nRegister the DataFrame as a SQL temporary view: source"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Load sample data from azure open dataset\n",
        "val hol_blob_account_name = \"azureopendatastorage\"\n",
        "val hol_blob_container_name = \"holidaydatacontainer\"\n",
        "val hol_blob_relative_path = \"Processed\"\n",
        "val hol_blob_sas_token = \"\"\n",
        "\n",
        "val hol_wasbs_path = f\"wasbs://$hol_blob_container_name@$hol_blob_account_name.blob.core.windows.net/$hol_blob_relative_path\"\n",
        "spark.conf.set(f\"fs.azure.sas.$hol_blob_container_name.$hol_blob_account_name.blob.core.windows.net\",hol_blob_sas_token)\n",
        "\n",
        "val hol_df = spark.read.parquet(hol_wasbs_path) \n",
        "\n",
        "println(\"Register the DataFrame as a SQL temporary view: source\")\n",
        "hol_df.createOrReplaceTempView(\"source\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "holiday_nodate: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n+---------------+--------------------------+--------------------------+-------------+-----------------+\n|countryOrRegion|holidayName               |normalizeHolidayName      |isPaidTimeOff|countryRegionCode|\n+---------------+--------------------------+--------------------------+-------------+-----------------+\n|Argentina      |Año Nuevo [New Year's Day]|Año Nuevo [New Year's Day]|null         |AR               |\n|Australia      |New Year's Day            |New Year's Day            |null         |AU               |\n|Austria        |Neujahr                   |Neujahr                   |null         |AT               |\n|Belgium        |Nieuwjaarsdag             |Nieuwjaarsdag             |null         |BE               |\n|Brazil         |Ano novo                  |Ano novo                  |null         |BR               |\n+---------------+--------------------------+--------------------------+-------------+-----------------+\nonly showing top 5 rows"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Remove datetime from the data source\n",
        "val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\n",
        "holiday_nodate.show(5,truncate = false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write a Spark dataframe into your sql pool\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "import org.apache.spark.sql.SqlAnalyticsConnector._\nimport com.microsoft.spark.sqlanalytics.utils.Constants\nsql_pool_name: String = mysqlpool\naccount_name: String = ruxune\ntemp_folder: String = data"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Write the dataframe into your sql pool\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\n",
        "\n",
        "val sql_pool_name = \"Your sql pool name\" //fill in your sql pool name\n",
        "\n",
        "holiday_nodate.write\n",
        "    .sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\n"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now open Synapse object explorer and go to **Data**->**Databases**->**<your sql pool name>**->**Tables**, you will see the new **dbo.PublicHoliday** table show up there."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read from a SQL Pool table with Spark\n",
        "\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "spark_read: org.apache.spark.sql.DataFrame = [countryOrRegion: string, holidayName: string ... 3 more fields]\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|countryOrRegion|holidayName              |normalizeHolidayName     |isPaidTimeOff|countryRegionCode|\n+---------------+-------------------------+-------------------------+-------------+-----------------+\n|Czech          |Den české státnosti      |Den české státnosti      |null         |CZ               |\n|Norway         |Søndag                   |Søndag                   |null         |NO               |\n|Sweden         |Söndag                   |Söndag                   |null         |SE               |\n|India          |Gandhi Jayanti           |Gandhi Jayanti           |true         |IN               |\n|Germany        |Tag der Deutschen Einheit|Tag der Deutschen Einheit|null         |DE               |\n+---------------+-------------------------+-------------------------+-------------+-----------------+\nonly showing top 5 rows"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Read  the table we just created in the sql pool as a Spark dataframe\n",
        "val spark_read = spark.read.\n",
        "    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\n",
        "spark_read.show(5, truncate = false)"
      ],
      "attachments": {}
    }
  ],
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
