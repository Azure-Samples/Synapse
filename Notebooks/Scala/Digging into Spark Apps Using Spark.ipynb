{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inception - Digging into Spark Apps Using Spark\n",
        "\n",
        "Apache Spark is a complex big data engine. At scale, failure become more of a norm than an exception. When your application fails, knowing what logs exist and how to take a look will help you debug and arrive at the root cause.\n",
        "\n",
        "When writing Spark applications, there are three kinds of logs to pay attention to:\n",
        "\n",
        "  1. Event Logs\n",
        "  2. Driver Logs\n",
        "  3. Executor Logs\n",
        "\n",
        "In this notebook, we will cover digging into Spark logs."
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Event Logs\n",
        "\n",
        "The first half of notebook assists you in analyzing Event Log of Spark application. \n",
        "\n",
        "* Event Log json file is read to a dataframe.\n",
        "* Useful queries are provided to obtain event related insight. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "res2: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6150d996"
          },
          "execution_count": 3,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Spin up a spark session.\n",
        "spark"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the path of Event Log file and read the file.\n",
        "A sample Event Log file is used to create this notebook. You have to set the path to your event log file accordingly. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "eventLogPath: String = <your-event-log-path>\neventLogDf: org.apache.spark.sql.DataFrame = [App Attempt ID: string, App ID: string ... 38 more fields]"
          },
          "execution_count": 4,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set the path of Event Log file.\n",
        "val eventLogPath = \"<your-event-log-path>\"\n",
        "    \n",
        "// Read the Event Log file using spark json reader.\n",
        "val eventLogDf = spark.read.json(eventLogPath)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "// Uncomment the following line if you want to print the highly nested schema of eventLogDf. \n",
        "// eventLogDf.printSchema"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-----------------------------------------------------------------+-----+\n|Event                                                            |count|\n+-----------------------------------------------------------------+-----+\n|org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd   |2    |\n|SparkListenerTaskStart                                           |44   |\n|SparkListenerBlockManagerAdded                                   |3    |\n|SparkListenerJobStart                                            |3    |\n|SparkListenerStageCompleted                                      |5    |\n|SparkListenerJobEnd                                              |3    |\n|SparkListenerApplicationEnd                                      |1    |\n|com.microsoft.peregrine.spark.listeners.PlanLogEvent             |2    |\n|org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates|2    |\n|SparkListenerLogStart                                            |1    |\n|SparkListenerExecutorAdded                                       |2    |\n|org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart |2    |\n|SparkListenerAggregatedLogPath                                   |1    |\n|SparkListenerEnvironmentUpdate                                   |1    |\n|SparkListenerStageSubmitted                                      |5    |\n|SparkListenerTaskEnd                                             |44   |\n|SparkListenerApplicationStart                                    |1    |\n+-----------------------------------------------------------------+-----+"
          },
          "execution_count": 6,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Check the number of different events.\n",
        "eventLogDf.select(\"Event\").groupBy(\"Event\").count.show(false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------------+\n|User                |\n+--------------------+\n|trusted-service-user|\n+--------------------+\n\n+------------------------------+\n|App ID                        |\n+------------------------------+\n|application_1587879306397_0001|\n+------------------------------+\n\n+------------------------------------------+\n|App Name                                  |\n+------------------------------------------+\n|AppName_1587879236530|\n+------------------------------------------+"
          },
          "execution_count": 7,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Check User, App ID and App Name.\n",
        "eventLogDf.select(\"User\").filter(eventLogDf.col(\"User\").isNotNull).show(false)\n",
        "eventLogDf.select(\"App ID\").filter(eventLogDf.col(\"App ID\").isNotNull).show(false)\n",
        "eventLogDf.select(\"App Name\").filter(eventLogDf.col(\"App Name\").isNotNull).show(false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "root\n |-- Driver Logs: struct (nullable = true)\n |    |-- stderr: string (nullable = true)\n |    |-- stdout: string (nullable = true)\n\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Driver Logs                                                                                                                                                                                                                                                                                                           |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[http://97f9b111926f4c0c9cb58d57d7d031cb000fe738074:8042/node/containerlogs/container_1587879306397_0001_01_000001/trusted-service-user/stderr?start=-4096, http://97f9b111926f4c0c9cb58d57d7d031cb000fe738074:8042/node/containerlogs/container_1587879306397_0001_01_000001/trusted-service-user/stdout?start=-4096]|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          },
          "execution_count": 8,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Find Driver Logs, including url for stderr and stdout.\n",
        "eventLogDf.select(\"Driver Logs\").printSchema\n",
        "eventLogDf.select(\"Driver Logs\").filter(eventLogDf.col(\"Driver Logs\").isNotNull).show(false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-----------+\n|Executor ID|\n+-----------+\n|          1|\n|          2|\n+-----------+\n\nroot\n |-- Executor Info: struct (nullable = true)\n |    |-- Host: string (nullable = true)\n |    |-- Log Urls: struct (nullable = true)\n |    |    |-- stderr: string (nullable = true)\n |    |    |-- stdout: string (nullable = true)\n |    |-- Total Cores: long (nullable = true)\n\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Executor Info                                                                                                                                                                                                                                                                                                                                                           |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[97f9b111926f4c0c9cb58d57d7d031cb000fe738074, [http://97f9b111926f4c0c9cb58d57d7d031cb000fe738074:8042/node/containerlogs/container_1587879306397_0001_01_000002/trusted-service-user/stderr?start=-4096, http://97f9b111926f4c0c9cb58d57d7d031cb000fe738074:8042/node/containerlogs/container_1587879306397_0001_01_000002/trusted-service-user/stdout?start=-4096], 4]|\n|[97f9b111926f4c0c9cb58d57d7d031cb002e9f47561, [http://97f9b111926f4c0c9cb58d57d7d031cb002e9f47561:8042/node/containerlogs/container_1587879306397_0001_01_000003/trusted-service-user/stderr?start=-4096, http://97f9b111926f4c0c9cb58d57d7d031cb002e9f47561:8042/node/containerlogs/container_1587879306397_0001_01_000003/trusted-service-user/stdout?start=-4096], 4]|\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          },
          "execution_count": 9,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Find the number of Executors and Executor Info, which includes Host ID, Log Urls and Total Cores.\n",
        "eventLogDf.select(\"Executor ID\").filter(eventLogDf.col(\"Executor ID\").isNotNull).show()\n",
        "eventLogDf.select(\"Executor Info\").printSchema\n",
        "eventLogDf.select(\"Executor Info\").filter(eventLogDf(\"Executor Info\").isNotNull).show(false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------+\n|Maximum Memory|\n+--------------+\n|15845975654   |\n+--------------+\n\n+---------------------+\n|Maximum Onheap Memory|\n+---------------------+\n|15845975654          |\n+---------------------+\n\n+----------------------+\n|Maximum Offheap Memory|\n+----------------------+\n|0                     |\n+----------------------+"
          },
          "execution_count": 10,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Show memory configuration, including Maximum Memory, Maximum Onheap Memory and Maximum Offheap Memory.\n",
        "eventLogDf.select(\"Maximum Memory\").filter(eventLogDf(\"Maximum Memory\").isNotNull).distinct().show(false)\n",
        "eventLogDf.select(\"Maximum Onheap Memory\").filter(eventLogDf(\"Maximum Onheap Memory\").isNotNull).distinct().show(false)\n",
        "eventLogDf.select(\"Maximum Offheap Memory\").filter(eventLogDf(\"Maximum Offheap Memory\").isNotNull).distinct().show(false)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing Spark App Driver and Executor Logs\n",
        "\n",
        "The second half of notebook notebook assists you in analyzing Spark application driver or executor log.\n",
        "* Driver or executor log text file is read as a dataframe.\n",
        "* The dataframe is transformed that it has two columns \"Timestamp\" and \"Message\".\n",
        "* A simple query is provided to filter log containing possible error message. "
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Set the path of driver or executor log file and read the file.\n",
        "A sample driver stderr log file is used when creating this notebook. You have to change the path to your driver or executor log file accordingly."
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "driverLogPath: String = <your-driver-or-executor-log-path>\ndriverLogDf: org.apache.spark.sql.DataFrame = [value: string]"
          },
          "execution_count": 11,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Set the path of log file.\n",
        "val driverLogPath = \"<your-driver-or-executor-log-path>\"\n",
        "// Read the Log file using spark text reader.\n",
        "val driverLogDf = spark.read.text(driverLogPath)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "splitingLine: String => (String, String)\nsplitingUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StructType(StructField(_1,StringType,true), StructField(_2,StringType,true)),Some(List(StringType)))"
          },
          "execution_count": 12,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Define a function that split the log line into timestamp and message.\n",
        "def splitingLine: (String => (String, String)) = {\n",
        "    // timestamp eg: \"20/04/26 05:35:39\"\n",
        "    val timestampPattern = raw\"(\\d{2})/(\\d{2})/(\\d{2})(\\s)(\\d{2}):(\\d{2}):(\\d{2})\".r\n",
        "    val timestampLength = 17\n",
        "    line => {\n",
        "        val filler = \" \" * timestampLength\n",
        "        if(line.length < timestampLength){\n",
        "            (filler, line)            \n",
        "        } else {\n",
        "            val prefix = line.substring(0, timestampLength)\n",
        "            prefix match {\n",
        "                case timestampPattern(_*) => (prefix, line.substring(timestampLength))\n",
        "                case _ => (filler, line)\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Create an UDF based on the splitingLine function.\n",
        "val splitingUDF = udf(splitingLine)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "transformedDf: org.apache.spark.sql.DataFrame = [value: string, newCol: struct<_1: string, _2: string>]\nfinalDriverLogDf: org.apache.spark.sql.DataFrame = [Timestamp: string, Message: string]\n+-----------------+--------------------+\n|        Timestamp|             Message|\n+-----------------+--------------------+\n|                 |Container: contai...|\n|                 |LogAggregationTyp...|\n|                 |=================...|\n|                 |      LogType:stderr|\n|                 |LogLastModifiedTi...|\n|                 |     LogLength:99632|\n|                 |        LogContents:|\n|                 |SLF4J: Class path...|\n|                 |SLF4J: Found bind...|\n|                 |SLF4J: Found bind...|\n|                 |SLF4J: See http:/...|\n|                 |SLF4J: Actual bin...|\n|20/04/26 05:35:39| INFO SignalUtils...|\n|20/04/26 05:35:39| INFO SignalUtils...|\n|20/04/26 05:35:39| INFO SignalUtils...|\n|20/04/26 05:35:40| INFO SecurityMan...|\n|20/04/26 05:35:40| INFO SecurityMan...|\n|20/04/26 05:35:40| INFO SecurityMan...|\n|20/04/26 05:35:40| INFO SecurityMan...|\n|20/04/26 05:35:40| INFO SecurityMan...|\n+-----------------+--------------------+\nonly showing top 20 rows"
          },
          "execution_count": 13,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Generate a driver log based dataframe with only two columns: Timestamp and Message.\n",
        "val transformedDf = driverLogDf.withColumn(\"newCol\", splitingUDF(driverLogDf(\"value\")))\n",
        "val finalDriverLogDf = transformedDf.select(transformedDf(\"newCol._1\").as(\"Timestamp\"), transformedDf(\"newCol._2\").as(\"Message\"))\n",
        "\n",
        "finalDriverLogDf.show()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|Timestamp        |Message                                                                                                                                                                              |\n+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                 |      -XX:OnOutOfMemoryError='kill %p' \\                                                                                                                                             |\n|20/04/26 06:26:19| INFO YarnAllocator: Executor for container container_1587879306397_0001_01_000003 exited because of a YARN event (e.g., pre-emption) and not because of an error in the running job.|\n|20/04/26 06:26:19| INFO YarnAllocator: Executor for container container_1587879306397_0001_01_000002 exited because of a YARN event (e.g., pre-emption) and not because of an error in the running job.|\n+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+"
          },
          "execution_count": 14,
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "// Find all the messages containning \"Error\" or \"error\". \n",
        "finalDriverLogDf.filter(finalDriverLogDf(\"Message\").contains(\"error\") || finalDriverLogDf(\"Message\").contains(\"Error\")).show(false)"
      ],
      "attachments": {}
    }
  ]
}