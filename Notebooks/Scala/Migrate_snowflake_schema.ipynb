{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Migrate schema from Snowflake to Synapse SQL Dedicated Pool\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "### This notebooks demos how to connect to Snowflake, read data from a schema named INFORMATION_SCHEMA, gather the list of the tables for the given schema and move those tables to a Synapse SQL dedicated pool.\r\n",
        "\r\n",
        "<ul>\r\n",
        "<li> Define connection source </li>\r\n",
        "<li> Specify connection options for the Snowflake instance </li>\r\n",
        "<li> Read Snowflake Information_schema.tables to compile list of tables for our schema\r\n",
        "    <ul>\r\n",
        "    <li> Read each Snowflake table into into Spark DataFrame </li> \r\n",
        "    <li> Write DataFrame to table to Synapse SQL Dedicated pool\r\n",
        "</ul>\r\n",
        "    </li>\r\n",
        "</ul>    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// To use Snowflake as a data source in Spark, use the .format option to provide the Snowflake connector class name that defines the data source.\r\n",
        "// Please note that you need to add spark-snowflake_2.12-2.9.0-spark_3.1.jar and snowflake-jdbc-3.13.6.jar to workspace packages as well as to cluster/session packages\r\n",
        "// You can download those jar files from https://mvnrepository.com/artifact/net.snowflake/spark-snowflake?repo=snowflakedb and https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc respectevly\r\n", 
	"// You can find instructions how to add customized jars to cluster/session packages at  https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-manage-scala-packages\r\n",      
	"\r\n",      
        "import net.snowflake.spark.snowflake.Utils.SNOWFLAKE_SOURCE_NAME\r\n",
        "val SNOWFLAKE_SOURCE_NAME = \"net.snowflake.spark.snowflake\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "DemoCluster",
              "session_id": 17,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-09T14:14:57.9353514Z",
              "session_start_time": "2021-08-09T14:14:57.98765Z",
              "execution_start_time": "2021-08-09T14:17:14.6699123Z",
              "execution_finish_time": "2021-08-09T14:17:15.2227281Z"
            },
            "text/plain": "StatementMeta(DemoCluster, 17, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// setting default paramter \r\n",
        "\r\n",
        "val sfschema=\"existing schema\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// To not expose snowflake credentials, it is best practice to store user, password and account in Azure Key Vault service in your suscription\r\n",
	"// Please reference https://docs.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault how to set up secrets in Azure Key Vault service\r\n",      
        "// Please note that you need to link your Azure Key Vault service (AKV) to your Synapse workspace  \r\n",
        "// mssparkutils package let you retrive your secrets from AKV\r\n",
        "\r\n",
        "val user = mssparkutils.credentials.getSecret(\"Azure Key Vault name \", \"secret name for user\",\"linked service name\")\r\n",
        "val password = mssparkutils.credentials.getSecret(\"Azure Key Vault name \", \"secret name for password\",\"linked service name\")\r\n",
        "val account = mssparkutils.credentials.getSecret(\"Azure Key Vault name \", \"secret name for account\",\"linked service name\")\r\n",
        "val account_URL = \"https://\" + account + \".azure.snowflakecomputing.com\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T17:58:15.5479493Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T17:58:15.7178403Z",
              "execution_finish_time": "2021-08-04T17:58:23.2651865Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 2, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 79,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// set up options to connect to Snowflake schema public on TESTDB database\r\n",
        " \r\n",
        "val sfoptions = Map( \r\n",
        "  \"sfUrl\" -> account_URL,\r\n",
        "  \"sfUser\"->user,\r\n",
        "  \"sfPassword\"-> password,\r\n",
        "  \"sfDatabase\"-> \"TESTDB\",\r\n",
        "  \"sfSchema\"-> \"PUBLIC\",\r\n",
        "  \"sfWarehouse\"-> \"COMPUTE_WH\"\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T17:58:26.7006748Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T17:58:26.8059265Z",
              "execution_finish_time": "2021-08-04T17:58:28.1653268Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sfoptions: scala.collection.immutable.Map[String,String] = Map(sfUrl -> https://yg58220.east-us-2.azure.snowflakecomputing.com, sfSchema -> PUBLIC, sfPassword -> <password>, sfUser -> <user>, sfWarehouse -> <warehouse>, sfDatabase -> TESTDB)\n"
          ]
        }
      ],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// Setup options to connect to schema INFORMATION_SCHEMA. That schema in Snowflake contains your database metadata\r\n",
        " \r\n",
        "val sfoptions1 = Map( \r\n",
        "  \"sfUrl\" -> account_URL,\r\n",
        "  \"sfUser\"->user,\r\n",
        "  \"sfPassword\"-> password,\r\n",
        "  \"sfDatabase\"-> \"TESTDB\",\r\n",
        "  \"sfSchema\"-> \"INFORMATION_SCHEMA\",\r\n",
        "  \"sfWarehouse\"-> \"COMPUTE_WH\"\r\n",
        ")\r\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T17:58:34.7448636Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T17:58:34.8537732Z",
              "execution_finish_time": "2021-08-04T17:58:36.0081044Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sfoptions1: scala.collection.immutable.Map[String,String] = Map(sfUrl -> https://yg58220.east-us-2.azure.snowflakecomputing.com, sfSchema -> INFORMATION_SCHEMA, sfPassword -> <password>, sfUser -> <user>, sfWarehouse -> <warehouse>, sfDatabase -> TESTDB)\n"
          ]
        }
      ],
      "execution_count": 81,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true,
        "tags": []
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// read table INFORMATION_SCHEMA.TABLES into a DataFrame. We need it to compile list of the tables within our schema\r\n",
        "\r\n",
        "val df_tl=spark.read.format( SNOWFLAKE_SOURCE_NAME ).options(sfoptions1).option(\"dbtable\",\"TABLES\").load()\r\n",
        "//display(df_tl)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T17:58:42.8787864Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T17:58:42.9973772Z",
              "execution_finish_time": "2021-08-04T17:58:48.579489Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructuredStream-spark package version: 3.0.0-2.1.1\n",
            "df_tl: org.apache.spark.sql.DataFrame = [TABLE_CATALOG: string, TABLE_SCHEMA: string ... 20 more fields]\n"
          ]
        }
      ],
      "execution_count": 82,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "// For easy iteration, convert selected info from DataFrame to collection\r\n",
        "val df_tab_list = df_tl.select(\"table_schema\", \"table_name\").filter(\"table_schema='PUBLIC'\").collect()\r\n",
        "//println(df_tab_list)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T18:02:25.0054968Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T18:02:25.209112Z",
              "execution_finish_time": "2021-08-04T18:02:34.1314757Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_tab_list: Array[org.apache.spark.sql.Row] = Array([PUBLIC,CUSTOMERT], [PUBLIC,CUSTOMER_TEST], [PUBLIC,NATIONT], [PUBLIC,REGIONT])\n",
            "[Lorg.apache.spark.sql.Row;@6d6b91a4\n"
          ]
        }
      ],
      "execution_count": 86,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Note: \r\n",
					"We are using df.write.synapsesql method to populate table in SQL dedicated pool. If your target schema anything but \"dbo\", it need to be exist before.\r\n",
					"At the same time this target schema should not have the table with name specified in this method. Here is stored procedure you can run to make sure that this requirement is met:\r\n",
					"\r\n",
					"```sql\r\n",
					"CREATE PROCEDURE set_sfschema  @schemaname sysname\r\n",
					"AS BEGIN\r\n",
					"    DECLARE @cr_stmt NVARCHAR(200) = N'CREATE SCHEMA ' + @schemaname; \r\n",
					"    -- to imulate cursor processing\r\n",
					"    CREATE TABLE #temp_tbl\r\n",
					"    WITH\r\n",
					"     ( DISTRIBUTION = ROUND_ROBIN\r\n",
					"      )\r\n",
					"       AS \r\n",
					"           SELECT  ROW_NUMBER() OVER(ORDER BY (SELECT NULL)) AS Sequence,\r\n",
					"              table_schema , table_name ,\r\n",
					"              'DROP TABLE ' +  quotename(table_schema) + '.' + quotename(table_name) as sql_code\r\n",
					"              from information_schema.tables WHERE table_schema = @schemaname ; \r\n",
					"    \r\n",
					"    DECLARE @nbr_statements INT = (SELECT COUNT(*) FROM #temp_tbl)\r\n",
					"    ,       @i INT = 1;\r\n",
					"\r\n",
					"    IF (0 = (SELECT COUNT(*) FROM sys.schemas WHERE name = @schemaname))\r\n",
					"      BEGIN\r\n",
					"          EXEC sp_executesql @tsql = @cr_stmt;\r\n",
					"      END\r\n",
					"    ELSE \r\n",
					"       WHILE   @i <= @nbr_statements\r\n",
					"          BEGIN\r\n",
					"              DECLARE @sql_code NVARCHAR(60) = (SELECT sql_code FROM #temp_tbl WHERE  Sequence =@i);\r\n",
					"              EXEC sp_executesql @sql_code;\r\n",
					"              SET     @i +=1;\r\n",
					"           END\r\n",
					"    DROP TABLE #temp_tbl; \r\n",
					"END\r\n",
					"GO\r\n",
					"```\r\n",
					"\r\n",
					""
				]
			},
    {
      "cell_type": "code",
      "source": [
        "// For each table in the schema read data from Snowflake table into a DataFrame and write it to Synapse SQL Dedicated Pool.\r\n",
        "\r\n",
        "df_tab_list.foreach(row=>\r\n",
        "   {\r\n",
        "    val tname = row.getString(1) \r\n",
        "     //println(tname)\r\n",
        "    val df_temp=spark.read.format( SNOWFLAKE_SOURCE_NAME ).options(sfoptions).option(\"dbtable\",tname).load()\r\n",
        "    val target_table = \"SQLdedpool1.\" +  sfschema  + \".\" + tname\r\n",
        "    println(target_table)\r\n",
        "    df_temp.write.synapsesql(target_table, Constants.INTERNAL)\r\n",
        "  })"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "TestS3Cluster",
              "session_id": 18,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-08-04T18:02:57.9134366Z",
              "session_start_time": null,
              "execution_start_time": "2021-08-04T18:02:58.0345256Z",
              "execution_finish_time": "2021-08-04T18:04:43.7619937Z"
            },
            "text/plain": "StatementMeta(TestS3Cluster, 18, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUSTOMERT\n",
            "CUSTOMER_TEST\n",
            "NATIONT\n",
            "REGIONT\n"
          ]
        }
      ],
      "execution_count": 87,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_spark",
      "language": "Scala",
      "display_name": "Synapse Spark"
    },
    "language_info": {
      "name": "scala"
    },
    "kernel_info": {
      "name": "synapse_spark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
