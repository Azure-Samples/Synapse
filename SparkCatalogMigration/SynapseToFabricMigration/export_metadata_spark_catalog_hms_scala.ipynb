{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var IntermediateFolderPath = \"abfss://<container>@<storage_name>.dfs.core.windows.net/hms_output/syn/\"\n",
        "var StorageAccountName = \"<storage_name>\"\n",
        "var StorageAccountAccessKey = \"<storage_account_key>\"\n",
        "\n",
        "var DatabaseNames = \"<db1_name>;<db2_name>\"\n",
        "var SkipExportTablesWithUnrecognizedType:Boolean = false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\n",
        "  \"fs.azure.account.key.\" + StorageAccountName + \".dfs.core.windows.net\",\n",
        "  StorageAccountAccessKey\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import java.net.URI\n",
        "import java.util.Calendar\n",
        "\n",
        "import scala.collection.mutable.{ListBuffer, Map, Set}\n",
        "import org.apache.spark.sql._\n",
        "import org.apache.spark.sql.types._\n",
        "import org.apache.spark.sql.catalyst._\n",
        "import org.apache.spark.sql.catalyst.analysis._\n",
        "import org.apache.spark.sql.catalyst.catalog._\n",
        "import org.json4s._\n",
        "import org.json4s.JsonAST.JString\n",
        "import org.json4s.jackson.Serialization\n",
        "\n",
        "object ExportMetadata {\n",
        "\n",
        "  lazy val spark = SparkSession\n",
        "    .builder()\n",
        "    .getOrCreate()\n",
        "\n",
        "  import spark.implicits._\n",
        "\n",
        "  val DatabaseType = \"database\"\n",
        "  val TableType = \"table\"\n",
        "  val PartitionType = \"partition\"\n",
        "\n",
        "  case class CatalogPartitions(database: String, table: String, tablePartitons: Seq[CatalogTablePartition])\n",
        "\n",
        "  case class CatalogTables(database: String, tables: Seq[CatalogTable])\n",
        "\n",
        "  case class CatalogStat(entityType: String, count: Int, database: Option[String], table: Option[String])\n",
        "  \n",
        "  case class NotExportedTable(database: String, table: String, error: String)\n",
        "\n",
        "  def ConvertToJsonStringList(objs: List[Object]):List[String] = {\n",
        "\n",
        "    case object URISerializer extends CustomSerializer[URI](format => ( {\n",
        "      case JString(uri) => new URI(uri)\n",
        "    }, {\n",
        "      case uri: URI => JString(uri.toString())\n",
        "    }))\n",
        "\n",
        "    case object SturctTypeSerializer extends CustomSerializer[StructType](format => ( {\n",
        "      case JString(structType)  => DataType.fromJson(structType).asInstanceOf[StructType]\n",
        "    }, {\n",
        "      case structType: StructType => JString(structType.json)\n",
        "    }))\n",
        "\n",
        "    implicit val formats = DefaultFormats + URISerializer + SturctTypeSerializer\n",
        "\n",
        "    var stringBuffer = new ListBuffer[String]()\n",
        "\n",
        "    objs.foreach(obj => {\n",
        "      stringBuffer += Serialization.write(obj)\n",
        "    })\n",
        "\n",
        "    return stringBuffer.toList\n",
        "  }\n",
        "\n",
        "  def WriteToFile(content:Seq[String], filePath: String) : Unit = {\n",
        "    val df = content.toDF\n",
        "    df.write.mode(SaveMode.Overwrite).text(filePath);\n",
        "  }\n",
        "\n",
        "  def ExportCatalogObjectsToFile(databases: List[CatalogDatabase], tables: List[CatalogTables], partitions: List[CatalogPartitions], stats: List[CatalogStat], notExportedTables: List[NotExportedTable], outputDirectory: String) : Unit = {\n",
        "    val jsonStringForDbs = ConvertToJsonStringList(databases)\n",
        "    WriteToFile(jsonStringForDbs, outputDirectory.trim() + \"/databases\")\n",
        "    println(\"Databases are exported to: \" + outputDirectory.trim() + \"databases \"+ Calendar.getInstance().getTime())\n",
        "\n",
        "    val jsonStringForTables = ConvertToJsonStringList(tables)\n",
        "    WriteToFile(jsonStringForTables, outputDirectory.trim() + \"/tables\")\n",
        "    println(\"Tables are exported to: \" + outputDirectory.trim() + \"tables \"+ Calendar.getInstance().getTime())\n",
        "\n",
        "    val jsonStringForParts = ConvertToJsonStringList(partitions)\n",
        "    WriteToFile(jsonStringForParts, outputDirectory.trim() + \"/partitions\")\n",
        "    println(\"Partitions are exported to: \" + outputDirectory.trim() + \"partitions \"+ Calendar.getInstance().getTime())\n",
        "\n",
        "    val jsonStringForStats = ConvertToJsonStringList(stats);\n",
        "    WriteToFile(jsonStringForStats, outputDirectory.trim() + \"/catalogObjectStats\")\n",
        "    \n",
        "    val jsonStringForNotExportedTables = ConvertToJsonStringList(notExportedTables);\n",
        "    WriteToFile(jsonStringForNotExportedTables, outputDirectory.trim() + \"notExportedTables\")\n",
        "  }\n",
        "\n",
        "  def ExportCatalogObjectFromMetadataStore(outputDirecoty: String, databaseNames: String):Unit = {\n",
        "    val maxObjectCount = 1000\n",
        "\n",
        "    var dbBuffer = new ListBuffer[CatalogDatabase]()\n",
        "    var tableBuffer = new ListBuffer[CatalogTables]()\n",
        "    var partitionBuffer = new ListBuffer[CatalogPartitions]()\n",
        "    var notExportedTableBuffer = new ListBuffer[NotExportedTable]()\n",
        "\n",
        "    var dbNames = spark.sharedState.externalCatalog.listDatabases()\n",
        "\n",
        "    var exportedDbName:Seq[String] = Seq()\n",
        "    if (databaseNames.nonEmpty) {\n",
        "      exportedDbName = databaseNames.split(\";\").filter(_.nonEmpty).map(db => db.trim())\n",
        "    }\n",
        "\n",
        "    var tableIds = Map[String, Seq[String]]()\n",
        "    var totalTableCount:Int = 0\n",
        "    dbNames.foreach( dbName => {\n",
        "      if (exportedDbName.contains(\"*\") || exportedDbName.contains(dbName)) {\n",
        "        try {\n",
        "          dbBuffer += spark.sharedState.externalCatalog.getDatabase(dbName)\n",
        "          val tableNames = spark.sharedState.externalCatalog.listTables(dbName)\n",
        "\n",
        "          tableIds.put(dbName, tableNames)\n",
        "          totalTableCount += tableNames.size\n",
        "        } catch {\n",
        "          case noSuchDbEx: NoSuchDatabaseException => {\n",
        "            println(\"Ignore not exists database '\" + dbName + \"' ex: \" + noSuchDbEx)\n",
        "          }\n",
        "          case ex:Exception => {\n",
        "            println(\"Failed to get database db = '\" + dbName + \"' with unexpected exception. ex: \" + ex);\n",
        "            throw ex;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    })\n",
        "\n",
        "    println(dbBuffer.size + \" databases get from metastore. \"+ Calendar.getInstance().getTime())\n",
        "    println(\"Totally \" + totalTableCount + \" tables will be exported.\")\n",
        "\n",
        "    var tableCount = 0;\n",
        "    var partitionCount = 0;\n",
        "    for( tableId <- tableIds) {\n",
        "      var dbName = tableId._1\n",
        "      var tables = new ListBuffer[CatalogTable]()\n",
        "\n",
        "      for(tableName <- tableId._2) {        \n",
        "        try\n",
        "        {          \n",
        "          val table = spark.sharedState.externalCatalog.getTable(dbName, tableName)        \n",
        "        \n",
        "          tables += table\n",
        "          tableCount += 1\n",
        "\n",
        "          if (table.partitionColumnNames.nonEmpty){\n",
        "            val tablePartitions =  spark.sharedState.externalCatalog.listPartitions(dbName, tableName)\n",
        "            partitionCount += tablePartitions.size\n",
        "\n",
        "            if (tablePartitions.nonEmpty) {\n",
        "\n",
        "              for (group <- tablePartitions.toList.grouped(maxObjectCount)) {\n",
        "                partitionBuffer += CatalogPartitions(table.identifier.database.get, table.identifier.table, group.toSeq)\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "\n",
        "          if (tableCount > 0 && tableCount%100 == 0) {\n",
        "            println(tableCount + \" tables get from metastore. \"+ Calendar.getInstance().getTime())\n",
        "            println(partitionCount + \" partitions get from metastore. \"+ Calendar.getInstance().getTime())\n",
        "          }\n",
        "        }\n",
        "        catch\n",
        "          {\n",
        "            case sparkEx: org.apache.spark.SparkException => {\n",
        "              var msg = sparkEx.getMessage\n",
        "              if (SkipExportTablesWithUnrecognizedType && msg.contains(\"Cannot recognize hive type string\")) {\n",
        "                println(\"Skip to export table. db = '\" + dbName + \"' table = '\" + tableName + \"'. ex: \" + sparkEx);\n",
        "                notExportedTableBuffer += NotExportedTable(dbName, tableName, msg)\n",
        "              } else {\n",
        "                throw sparkEx\n",
        "              }\n",
        "            }\n",
        "            case argEx:IllegalArgumentException => {\n",
        "              var msg = argEx.getMessage\n",
        "              if (SkipExportTablesWithUnrecognizedType && msg.contains(\"Failed to convert the JSON string\") && msg.contains(\"to a data type\")) {\n",
        "                println(\"Skip to export table. db = '\" + dbName + \"' table = '\" + tableName + \"'. ex: \" +  argEx);\n",
        "                notExportedTableBuffer += NotExportedTable(dbName, tableName, msg)\n",
        "              }\n",
        "            }\n",
        "            case noSuchTableEx: NoSuchTableException => {\n",
        "              println(\"Ignore not exists table. db = '\" + dbName + \"' table = '\" + tableName + \"'. ex: \" + noSuchTableEx)\n",
        "              notExportedTableBuffer += NotExportedTable(dbName, tableName, noSuchTableEx.getMessage)\n",
        "            }\n",
        "            case ex:Exception => {\n",
        "              println(\"Failed to get table db = \" + dbName + \" table = \" + tableName + \" with unexpected exception. ex: \" + ex);\n",
        "              throw ex;\n",
        "            }\n",
        "          }\n",
        "      }\n",
        "\n",
        "      for (group <- tables.toList.grouped(maxObjectCount)) {\n",
        "        tableBuffer += CatalogTables(dbName, group)\n",
        "      }\n",
        "    }\n",
        "\n",
        "    println(tableCount + \" tables get from metastore. \"+ Calendar.getInstance().getTime())\n",
        "    println(partitionCount + \" partitions get from metastore. \"+ Calendar.getInstance().getTime())\n",
        "    println(\"skip export \" + notExportedTableBuffer.size + \" tables due to failed to read them. \" + Calendar.getInstance().getTime())\n",
        "\n",
        "    var statBuffer = new ListBuffer[CatalogStat];\n",
        "    statBuffer.append(CatalogStat(DatabaseType, dbBuffer.size, None, None))\n",
        "\n",
        "    totalTableCount = 0;\n",
        "    tableBuffer.groupBy(tbls => tbls.database).foreach(group => {\n",
        "      var tblcount = 0;\n",
        "      group._2.foreach(tbls => {\n",
        "        tblcount += tbls.tables.size\n",
        "      })\n",
        "      statBuffer.append(CatalogStat(TableType, tblcount, Some(group._1), None))\n",
        "      totalTableCount += tblcount;\n",
        "    })\n",
        "    statBuffer.append(CatalogStat(TableType, totalTableCount, None, None))\n",
        "\n",
        "    var totablPartitionCount = 0;\n",
        "    partitionBuffer.groupBy(parts => (parts.database, parts.table)).foreach(group => {\n",
        "      var partCount = 0;\n",
        "      group._2.foreach(parts => {\n",
        "        partCount += parts.tablePartitons.size\n",
        "      })\n",
        "      statBuffer.append(CatalogStat(PartitionType, partCount, Some(group._1._1), Some(group._1._2)))\n",
        "      totablPartitionCount += partCount;\n",
        "    })\n",
        "    statBuffer.append(new CatalogStat(PartitionType, totablPartitionCount, None, None))\n",
        "\n",
        "    ExportCatalogObjectsToFile(dbBuffer.toList, tableBuffer.toList, partitionBuffer.toList, statBuffer.toList, notExportedTableBuffer.toList, outputDirecoty)\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "println(\"IntermediateFolderPath: \" + IntermediateFolderPath + \". \" + Calendar.getInstance().getTime())\n",
        "println(\"DatabaseNames : \" + DatabaseNames + \". \" + Calendar.getInstance().getTime())\n",
        "ExportMetadata.ExportCatalogObjectFromMetadataStore(IntermediateFolderPath, DatabaseNames)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
